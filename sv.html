<!DOCTYPE html>
<html lang="sv">
<head>
  <meta charset="UTF-8" />
  <title>TRepLiNa: Förbättrad maskinöversättning för lågresursspråk med CKA + REPINA</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <style>
    body {
      font-family: system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif;
      line-height: 1.7;
      margin: 0;
      padding: 0;
      background: #f7f7fb;
      color: #222;
    }
    header {
      background: #1b2430;
      color: #fff;
      padding: 1.5rem 1rem;
      display: flex;
      justify-content: space-between;
      align-items: center;
    }
    header h1 {
      margin: 0;
      font-size: 1.8rem;
    }
    .lang-switch {
      margin-right: 1rem;
    }
    .lang-switch a {
      background: #fff;
      color: #1b2430;
      padding: 6px 14px;
      border-radius: 8px;
      text-decoration: none;
      font-size: 0.9rem;
      font-weight: bold;
    }
    .lang-switch a:hover {
      background: #dce0ff;
    }
    main {
      max-width: 900px;
      margin: 0 auto;
      padding: 1.5rem 1rem 3rem;
      background: #fff;
    }
    h2, h3, h4 {
      margin-top: 2rem;
      color: #1b2430;
    }
    h2 {
      border-bottom: 2px solid #e0e0f0;
      padding-bottom: 0.3rem;
    }
    a {
      color: #0066cc;
      text-decoration: none;
    }
    a:hover {
      text-decoration: underline;
    }
    .paper-info {
      background: #f0f2ff;
      border-left: 4px solid #4c6fff;
      padding: 0.8rem 1rem;
      margin: 1rem 0;
      font-size: 0.95rem;
    }
    .math-note {
      font-size: 0.9rem;
      color: #555;
      background: #fafafa;
      border-left: 3px solid #ccc;
      padding: 0.6rem 0.8rem;
      margin-top: 0.5rem;
    }
    pre {
      background: #0f172a;
      color: #e5e7eb;
      padding: 1rem;
      overflow-x: auto;
      border-radius: 6px;
      font-size: 0.85rem;
    }
  </style>

  <!-- MathJax -->
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [["\\(", "\\)"], ["$", "$"]],
        displayMath: [["\\[", "\\]"], ["$$", "$$"]]
      }
    };
  </script>
  <script async id="MathJax-script"
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>
  <header>
    <div>
      <h1>TRepLiNa: förbättrar maskinöversättning för lågresursspråk med CKA + REPINA-alignment</h1>
      <p>Finjustering av Aya-23 8B för lågresursspråk baserad på lagvis representations­alignering</p>
    </div>

    <!-- Språkväxling -->
    <div class="lang-switch" id="lang-switch"></div>
  </header>

  <main>
    <section>
      <div class="paper-info">
        <span class="tag">Artikel</span>
        <strong>TRepLiNa: Layer-wise CKA+REPINA Alignment Improves Low-Resource Machine Translation in Aya-23 8B</strong><br />
        arXiv: <a href="https://arxiv.org/abs/2510.06249" target="_blank" rel="noopener noreferrer">
          https://arxiv.org/abs/2510.06249
        </a><br />
        Publiceringsår: 2025
      </div>

      <p>
        På den här sidan vill jag förklara TRepLiNa-artikeln. Denna artikel föreslår en metod
        som bygger på insikter från
        <strong>mechanistic interpretability (mekanistisk tolkningsbarhet)</strong>,
        ett forskningsområde som analyserar språkmodellers interna representationer
        och neuroners funktion, och använder dessa insikter för att förbättra
        maskinöversättning för lågresursspråk.
      </p>

      <p>
        Forskning inom mechanistic interpretability har till exempel visat att många stora
        språkmodeller, även om de tar emot många olika språk som indata, internt tenderar att
        först projicera dessa språk till ett representationsrum som liknar engelska, och
        därefter bearbeta informationen därifrån. Förenklat kan man säga att
        <strong>”språkmodellen i praktiken tänker på engelska internt”</strong>.
      </p>

      <p>
        När modellen ska lära sig ett lågresursspråk som den nästan aldrig har sett tidigare,
        är vägarna från detta språk till de interna representationer där modellen ”tänker
        på engelska” ännu inte ordentligt etablerade. TRepLiNa fokuserar på just denna punkt
        och inför
        <strong>representationslikheten mellan ett nytt språk och ett högresursspråk
        (här huvudsakligen engelska)</strong>
        i förlustfunktionen, i syfte att organisera vägen till dessa interna
        “engelsktänkande”-representationer bättre.
      </p>

      <p>
        Som resultat visar artikeln att modellen, i översättningsuppgifter för
        lågresursspråk, i flera fall kan uppnå högre BLEU- och chrF++-poäng än med
        vanlig finjustering
        (effekten beror dock också på faktorer som typologisk likhet mellan språken).
      </p>

      <p>
        Artikeln fokuserar främst på översättning i riktningen
        <strong>lågresursspråk → högresursspråk</strong> (t.ex. LRL → engelska),
        men de experimentella resultaten antyder också att metoden kan vara effektiv
        i riktningen <strong>högresursspråk → lågresursspråk</strong>.
      </p>
    </section>

    <!-- Mekanism -->
    <section>
      <h2>Översikt över metoden</h2>
      <p>
        I TRepLiNa kontrolleras
        <strong>likheten mellan interna representationer</strong>
        för låg- och högresursspråk med hjälp av en förlustfunktion som kombinerar
        följande två tekniker:
      </p>
      <ul>
        <li><strong>REPINA</strong>: en regularisering som begränsar driften mellan basmodellens och den finjusterade modellens representationer</li>
        <li><strong>CKA (Centered Kernel Alignment)</strong>: ett mått på hur lika representationsrummen är för två olika språk</li>
      </ul>

      <!-- REPINA -->
      <h3>Hur REPINA fungerar</h3>
      <p>
        REPINA är ursprungligen ett förslag på en
        <strong>representationsbevarande regularisering</strong>
        som kontrollerar ”hur mycket representationerna efter finjustering avviker
        från basmodellens representationer”.
      </p>

      <h4>Allmän bild (REPINA med linjär projektion)</h4>
      <p>
        För token \(i\) i lager \(\ell\) definierar vi:
      </p>
      <ul>
        <li>\(h_{\ell,i}^{(0)}\): dold tillståndsvektor i basmodellen (före finjustering)</li>
        <li>\(h_{\ell,i}^{(\theta)}\): dold tillståndsvektor efter finjustering (med parametrar \(\theta\))</li>
      </ul>
      <p>
        REPINA antar att det finns en linjär avbildning \(W\) mellan representationsrummet
        i basmodellen och representationsrummet i den finjusterade modellen, och lär
        \(W\) så att \(W h_{\ell,i}^{(\theta)}\) ligger så nära \(h_{\ell,i}^{(0)}\) som möjligt.
      </p>

      <p>
        REPINA-förlusten kan typiskt skrivas som:
      </p>
      <p class="math-note">
        \[
          \mathcal{L}_{\mathrm{REPINA}}
          = \frac{1}{N}
            \sum_{i=1}^{N}
              \left\lVert
                W h_{\ell,i}^{(\theta)} - h_{\ell,i}^{(0)}
              \right\rVert_2^2
        \]
      </p>

      <h4>REPINA<sup>I</sup> i denna implementering (identitetsprojektion)</h4>
      <p>
        I TRepLiNa-implementeringen används en enklare variant,
        <strong>REPINA<sup>I</sup> (identity projection)</strong>,
        där man sätter \(W = I\) (identitetsmatrisen). Då blir förlusten:
      </p>
      <p class="math-note">
        \[
          \mathcal{L}_{\mathrm{REPINA}^{I}}
          = \frac{1}{N}
            \sum_{i=1}^{N}
              \left\lVert
                h_{\ell,i}^{(\theta)} - h_{\ell,i}^{(0)}
              \right\rVert_2^2
        \]
      </p>
      <p>
        Med andra ord läggs en L2-straff på så att
        <strong>representationerna som erhållits efter finjustering med exempelvis LoRA</strong>
        inte driver för långt från
        <strong>bas-Aya-23:s representationer med adaptrarna avaktiverade</strong>.
      </p>

      <!-- CKA -->
      <h3>Hur CKA fungerar</h3>
      <p>
        CKA (Centered Kernel Alignment) är ett mått på
        <strong>hur lika två representationsmatriser är i fråga om ”form” eller ”struktur”</strong>.
        Särskilt är linjär CKA en relativt enkel och lättanvänd variant, föreslagen av
        Kornblith et al. (2019).
      </p>

      <h4>Definition av linjär CKA</h4>
      <p>
        Låt tokenrepresentationerna i ett givet lager samlas i matriserna
        \(X \in \mathbb{R}^{N \times D}\) och
        \(Y \in \mathbb{R}^{N \times D}\), där \(N\) är antalet token och \(D\)
        är dimensionen på den dolda representationen. Genom att subtrahera medelvärdet
        över raderna (över exemplen) får vi de centrerade matriserna
        \(\tilde{X}\) och \(\tilde{Y}\):
      </p>
      <p class="math-note">
        \[
          \mathrm{CKA}(X, Y)
          =
          \frac{
            \left\lVert \tilde{X}^\top \tilde{Y} \right\rVert_F^2
          }{
            \left\lVert \tilde{X}^\top \tilde{X} \right\rVert_F
            \cdot
            \left\lVert \tilde{Y}^\top \tilde{Y} \right\rVert_F
          }
        \]
      </p>
      <p>
        Här betecknar \(\lVert \cdot \rVert_F\) Frobeniusnormen.
        Värdet är normaliserat så att \(0 \leq \mathrm{CKA} \leq 1\), och
        <strong>värden nära 1 betyder att de två representationsrummen liknar varandra starkt</strong>.
      </p>

      <p>
        I TRepLiNa används representatonsmatrisen för lågresursspråket som \(X\)
        och matrisen för högresursspråket (pivåtspråket) som \(Y\), och CKA beräknas.
        Eftersom vi vill att
        <strong>”de två språkens representationer ska likna varandra”</strong>,
        minimeras \(1 - \mathrm{CKA}(X,Y)\) som förlust:
      </p>
      <p class="math-note">
        \[
          \mathcal{L}_{\mathrm{CKA}} = 1 - \mathrm{CKA}(X, Y)
        \]
      </p>
      <p>
        Det finns dock en risk: CKA kan dra de interna representationerna för
        högresursspråket mot de ännu inte välorganiserade representationerna för
        lågresursspråket och därmed skada de redan etablerade interna vägarna i modellen.
        I artikeln rapporteras att CKA blir mer effektiv när den används tillsammans
        med REPINA, eftersom REPINA skyddar basmodellens representationer och stabiliserar
        den interna strukturen.
      </p>
    </section>

    <!-- Implementationsdetaljer -->
    <section>
      <h2>Implementationsdetaljer (CKA / REPINA / total förlust)</h2>

      <p>
        Hela koden är publicerad på GitHub:
        <a href="https://github.com/konta3738/cka-repina-aya23" target="_blank" rel="noopener noreferrer">
          https://github.com/konta3738/cka-repina-aya23
        </a><br />
        Här plockar vi enbart ut de delar som är direkt relaterade till
        <strong>CKA-delen, REPINA-delen och den totala förlustfunktionen</strong>,
        baserat på den kod du har tillhandahållit.
      </p>

      <!-- CKA-implementering -->
      <h3>1. Implementering av CKA</h3>

      <pre><code>def center_rows(x: torch.Tensor, mask: Optional[torch.Tensor] = None) -&gt; torch.Tensor:
    """
    Center features along the sample dimension (rows).

    x: [N, D]
    mask: [N] boolean or float mask (1 for keep). If provided, compute mean over masked rows.
    """
    if mask is None:
        mean = x.mean(dim=0, keepdim=True)
        return x - mean
    else:
        if mask.dtype != torch.float32:
            mask = mask.float()
        # avoid division by zero
        denom = mask.sum().clamp(min=1.0)
        mean = (x * mask.unsqueeze(1)).sum(dim=0, keepdim=True) / denom
        return x - mean

def linear_cka(x: torch.Tensor, y: torch.Tensor,
               x_mask: Optional[torch.Tensor] = None,
               y_mask: Optional[torch.Tensor] = None,
               eps: float = 1e-6) -&gt; torch.Tensor:
    """
    Compute linear CKA between two representation matrices.

    x: [N, D]
    y: [M, D]  (we'll downsample/upsample to min(N,M) by truncation to align tokens if not equal)
    masks: optional [N] and [M] masks
    """
    # If different number of rows (tokens), align by truncation to min length.
    n = x.shape[0]
    m = y.shape[0]
    L = min(n, m)
    x = x[:L]
    y = y[:L]
    if x_mask is not None:
        x_mask = x_mask[:L]
    if y_mask is not None:
        y_mask = y_mask[:L]

    # Center rows
    x = center_rows(x, x_mask)
    y = center_rows(y, y_mask)

    # Following Kornblith et al. (2019): CKA = ||X^T Y||_F^2 / (||X^T X||_F * ||Y^T Y||_F)
    # where X, Y are row-centered.
    xty = x.T @ y
    num = (xty * xty).sum()

    xtx = x.T @ x
    yty = y.T @ y
    denom = torch.sqrt((xtx * xtx).sum().clamp(min=eps)) * \
            torch.sqrt((yty * yty).sum().clamp(min=eps))
    return (num / denom).clamp(min=0.0, max=1.0)</code></pre>

      <!-- REPINA-implementering -->
      <h3>2. Implementering av REPINA<sup>I</sup></h3>

      <pre><code># 3) REPINA^I (identity projection): L2 between base (adapters disabled)
# and current reps, same inputs as a_align
do_repina = ((global_step + 1) % 2 == 0)  # every 2 steps
if do_repina:
    with torch.no_grad():
        with model.disable_adapter():
            base_out = forward_hidden(model, a_ids, a_attn)

    base_reps_all, _ = gather_layer_hidden(base_out.hidden_states, layer_index, a_attn)
    rep_cur = a_reps_all[a_src_keep]
    rep_pre = base_reps_all[a_src_keep]
    if rep_cur.shape[0] != rep_pre.shape[0]:
        L = min(rep_cur.shape[0], rep_pre.shape[0])
        rep_cur = rep_cur[:L]
        rep_pre = rep_pre[:L]

    repina_loss = torch.mean((rep_cur - rep_pre) ** 2)</code></pre>

      <!-- Total förlustfunktion -->
      <h3>3. Total förlustfunktion</h3>

      <pre><code>def compute_losses(
    model,
    batch: Dict[str, torch.Tensor],
    tokenizer,
    layer_index: int,
    lambda_cka: float,
    mu_repina: float,
    bf16: bool,
    global_step
):
    """
    Returns (total_loss, dict of components)
    """
    device = next(model.parameters()).device
    repina_loss = torch.tensor(0.0, device=device)

    # 1) Task loss: LM on prompt+target (labels already mask prompt with -100)
    input_ids = batch["input_ids"].to(device)
    labels    = batch["labels"].to(device)
    attention_mask = (input_ids != tokenizer.pad_token_id).to(device)

    outputs = model(
        input_ids=input_ids,
        attention_mask=attention_mask,
        labels=labels,
        use_cache=False,
        output_hidden_states=True
    )
    task_loss = compute_task_loss_with_label_smoothing(
        outputs, labels, epsilon=0.1, ignore_index=-100
    )

    # 2) Alignment loss (CKA) on source regions of A and B
    a_ids = batch["a_align_ids"].to(device)
    b_ids = batch["b_align_ids"].to(device)

    a_attn = (a_ids != tokenizer.pad_token_id).to(device)
    b_attn = (b_ids != tokenizer.pad_token_id).to(device)

    a_out = forward_hidden(model, a_ids, a_attn)
    b_out = forward_hidden(model, b_ids, b_attn)

    a_reps_all, a_tokmask_all = gather_layer_hidden(a_out.hidden_states, layer_index, a_attn)
    b_reps_all, b_tokmask_all = gather_layer_hidden(b_out.hidden_states, layer_index, b_attn)

    a_src_keep = a_attn.reshape(-1).bool()
    b_src_keep = b_attn.reshape(-1).bool()

    a_src = a_reps_all[a_src_keep]
    b_src = b_reps_all[b_src_keep]

    if a_src.numel() == 0:
        a_src = a_reps_all[a_tokmask_all]
    if b_src.numel() == 0:
        b_src = b_reps_all[b_tokmask_all]

    cka_val = linear_cka(a_src, b_src, None, None)
    cka_loss = 1.0 - cka_val

    # 3) REPINA^I
    do_repina = ((global_step + 1) % 2 == 0)
    if do_repina:
        with torch.no_grad():
            with model.disable_adapter():
                base_out = forward_hidden(model, a_ids, a_attn)
        base_reps_all, _ = gather_layer_hidden(base_out.hidden_states, layer_index, a_attn)
        rep_cur = a_reps_all[a_src_keep]
        rep_pre = base_reps_all[a_src_keep]
        if rep_cur.shape[0] != rep_pre.shape[0]:
            L = min(rep_cur.shape[0], rep_pre.shape[0])
            rep_cur = rep_cur[:L]
            rep_pre = rep_pre[:L]

        repina_loss = torch.mean((rep_cur - rep_pre) ** 2)

        total = task_loss + lambda_cka * cka_loss + mu_repina * repina_loss
    else:
        total = task_loss + lambda_cka * cka_loss

    return total, {
        "task_loss": task_loss.detach().float().item(),
        "cka": cka_val.detach().float().item(),
        "cka_loss": cka_loss.detach().float().item(),
        "repina": repina_loss.detach().float().item(),
        "total": total.detach().float().item()
    }</code></pre>

      <h4>Sammanfattning i form av formler</h4>
      <p class="math-note">
        Om vi betecknar uppgiftsförlusten med \(\mathcal{L}_{\mathrm{task}}\),
        CKA-förlusten med \(\mathcal{L}_{\mathrm{CKA}} = 1 - \mathrm{CKA}(X,Y)\)
        och REPINA<sup>I</sup>-förlusten med
        \(\mathcal{L}_{\mathrm{REPINA}^{I}}\), kan den totala förlusten skrivas som
        \[
          \mathcal{L}_{\mathrm{total}}
          =
          \mathcal{L}_{\mathrm{task}}
          + \lambda_{\mathrm{CKA}} \cdot \mathcal{L}_{\mathrm{CKA}}
          + \mu_{\mathrm{REPINA}} \cdot \mathcal{L}_{\mathrm{REPINA}^{I}}
        \]
        (i implementeringen adderas REPINA-delen endast med vissa intervall, t.ex. med
        några stegs mellanrum under träningen).
      </p>
    </section>
  </main>

  <footer>
    TRepLiNa / CKA + REPINA-tuning – förklaringssida (HTML-version)
  </footer>
<script src="lang-switch.js"></script>
</body>
</html>
