<!DOCTYPE html>
<html lang="fr">
<head>
  <meta charset="UTF-8" />
  <title>TRepLiNa : amélioration de la traduction automatique à faibles ressources avec CKA + REPINA</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <style>
    body {
      font-family: system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif;
      line-height: 1.7;
      margin: 0;
      padding: 0;
      background: #f7f7fb;
      color: #222;
    }
    header {
      background: #1b2430;
      color: #fff;
      padding: 1.5rem 1rem;
      display: flex;
      justify-content: space-between;
      align-items: center;
    }
    header h1 {
      margin: 0;
      font-size: 1.8rem;
    }
    .lang-switch {
      margin-right: 1rem;
    }
    .lang-switch a {
      background: #fff;
      color: #1b2430;
      padding: 6px 14px;
      border-radius: 8px;
      text-decoration: none;
      font-size: 0.9rem;
      font-weight: bold;
    }
    .lang-switch a:hover {
      background: #dce0ff;
    }
    main {
      max-width: 900px;
      margin: 0 auto;
      padding: 1.5rem 1rem 3rem;
      background: #fff;
    }
    h2, h3, h4 {
      margin-top: 2rem;
      color: #1b2430;
    }
    h2 {
      border-bottom: 2px solid #e0e0f0;
      padding-bottom: 0.3rem;
    }
    a {
      color: #0066cc;
      text-decoration: none;
    }
    a:hover {
      text-decoration: underline;
    }
    .paper-info {
      background: #f0f2ff;
      border-left: 4px solid #4c6fff;
      padding: 0.8rem 1rem;
      margin: 1rem 0;
      font-size: 0.95rem;
    }
    .math-note {
      font-size: 0.9rem;
      color: #555;
      background: #fafafa;
      border-left: 3px solid #ccc;
      padding: 0.6rem 0.8rem;
      margin-top: 0.5rem;
    }
    pre {
      background: #0f172a;
      color: #e5e7eb;
      padding: 1rem;
      overflow-x: auto;
      border-radius: 6px;
      font-size: 0.85rem;
    }
  </style>

  <!-- MathJax -->
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [["\\(", "\\)"], ["$", "$"]],
        displayMath: [["\\[", "\\]"], ["$$", "$$"]]
      }
    };
  </script>
  <script async id="MathJax-script"
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>
  <header>
    <div>
      <h1>TRepLiNa : améliorer la traduction automatique à faibles ressources avec l’alignement CKA + REPINA</h1>
      <p>Réglage pour langues à faibles ressources d’Aya-23 8B via un alignement de représentations couche par couche</p>
    </div>

    <!-- Sélecteur de langue -->
    <div class="lang-switch">
      <a href="index.html">日本語</a>
      <a href="zh.html">中文</a>
    </div>
  </header>

  <main>
    <section>
      <div class="paper-info">
        <span class="tag">Article</span>
        <strong>TRepLiNa: Layer-wise CKA+REPINA Alignment Improves Low-Resource Machine Translation in Aya-23 8B</strong><br />
        arXiv : <a href="https://arxiv.org/abs/2510.06249" target="_blank" rel="noopener noreferrer">
          https://arxiv.org/abs/2510.06249
        </a><br />
        Année de publication : 2025
      </div>

      <p>
        Dans cette page, je vais présenter l’article TRepLiNa. Cet article propose une méthode
        qui applique à la traduction automatique pour les langues à faibles ressources des
        idées issues de la <strong>mechanistic interpretability (interprétabilité mécaniste)</strong>,
        c’est-à-dire un domaine qui analyse les représentations internes et le comportement des neurones
        dans les modèles de langue.
      </p>

      <p>
        D’après les travaux en mechanistic interpretability, de nombreux grands modèles de langue
        semblent <strong>recevoir des entrées dans des langues variées tout en mappant ces entrées,
        en interne, vers un espace de représentation proche de l’anglais avant de les traiter</strong>.
        En termes simples, on peut dire que
        <strong>« le modèle de langue pense en anglais à l’intérieur »</strong>.
      </p>

      <p>
        Lorsque le modèle apprend une langue à faibles ressources qu’il a très peu vue auparavant,
        le chemin qui relie cette langue aux <strong>représentations internes avec lesquelles il
        “pense en anglais”</strong> n’est pas encore bien organisé. TRepLiNa se focalise sur ce point
        et vise à renforcer ce chemin en ajoutant à la fonction de perte
        la <strong>similarité de représentation entre une langue nouvellement apprise et une langue
        à fortes ressources (principalement l’anglais ici)</strong>.
      </p>

      <p>
        En conséquence, pour des tâches de traduction automatique de langues à faibles ressources,
        l’article montre des cas où l’on obtient des scores BLEU / chrF++ plus élevés que
        ceux d’un simple fine-tuning standard (même si l’ampleur de l’effet dépend, entre autres,
        de la similarité typologique entre les langues).
      </p>

      <p>
        L’article se concentre principalement sur la direction de traduction
        <strong>langue à faibles ressources → langue à fortes ressources</strong> (par ex. LRL → anglais),
        mais les résultats expérimentaux suggèrent aussi que la méthode est bénéfique pour la direction
        <strong>langue à fortes ressources → langue à faibles ressources</strong>.
      </p>
    </section>

    <!-- Mécanisme -->
    <section>
      <h2>Vue d’ensemble du mécanisme</h2>
      <p>
        Dans TRepLiNa, pour contrôler la <strong>similarité des représentations internes</strong>
        entre une langue à faibles ressources et une langue à fortes ressources, on utilise
        une fonction de perte qui combine les deux techniques suivantes :
      </p>
      <ul>
        <li><strong>REPINA</strong> : une régularisation qui limite la dérive de représentation entre le modèle de base et le modèle affiné</li>
        <li><strong>CKA (Centered Kernel Alignment)</strong> : un indicateur qui mesure la similarité entre les espaces de représentation de langues différentes</li>
      </ul>

      <!-- REPINA -->
      <h3>Fonctionnement de REPINA</h3>
      <p>
        REPINA a été initialement proposée comme une <strong>régularisation préservant les représentations</strong>,
        destinée à contrôler <strong>dans quelle mesure les représentations après fine-tuning
        s’éloignent de celles du modèle de base</strong>.
      </p>

      <h4>Intuition générale (REPINA avec projection linéaire)</h4>
      <p>
        Pour un token \(i\) à la couche \(\ell\), on définit :
      </p>
      <ul>
        <li>\(h_{\ell,i}^{(0)}\) : l’état caché avant le fine-tuning (modèle de base)</li>
        <li>\(h_{\ell,i}^{(\theta)}\) : l’état caché après le fine-tuning (modèle avec paramètres \(\theta\))</li>
      </ul>
      <p>
        REPINA suppose qu’il existe une application linéaire \(W\) entre l’espace de représentation
        du modèle de base et celui du modèle affiné, et apprend \(W\) de façon à ce que
        \(W h_{\ell,i}^{(\theta)}\) soit aussi proche que possible de \(h_{\ell,i}^{(0)}\).
      </p>

      <p>
        La perte REPINA peut alors s’écrire typiquement :
      </p>
      <p class="math-note">
        \[
          \mathcal{L}_{\mathrm{REPINA}}
          = \frac{1}{N}
            \sum_{i=1}^{N}
              \left\lVert
                W h_{\ell,i}^{(\theta)} - h_{\ell,i}^{(0)}
              \right\rVert_2^2
        \]
      </p>

      <h4>REPINA<sup>I</sup> dans cette implémentation (projection identité)</h4>
      <p>
        Dans l’implémentation de TRepLiNa, on utilise une variante plus simple,
        <strong>REPINA<sup>I</sup> (version avec projection identité)</strong>, dans laquelle
        on pose \(W = I\) (la matrice identité). La perte devient alors :
      </p>
      <p class="math-note">
        \[
          \mathcal{L}_{\mathrm{REPINA}^{I}}
          = \frac{1}{N}
            \sum_{i=1}^{N}
              \left\lVert
                h_{\ell,i}^{(\theta)} - h_{\ell,i}^{(0)}
              \right\rVert_2^2
        \]
      </p>
      <p>
        Autrement dit, on pénalise, via une distance L2, les cas où
        les <strong>représentations après fine-tuning (par ex. avec LoRA)</strong>
        s’écartent trop des <strong>représentations du modèle Aya-23 de base
        avec les adapteurs désactivés</strong>.
      </p>

      <!-- CKA -->
      <h3>Fonctionnement de CKA</h3>
      <p>
        CKA (Centered Kernel Alignment) est une mesure indiquant
        <strong>dans quelle mesure la « forme » ou la « structure » de deux matrices de représentation
        se ressemblent</strong>. La version linéaire de CKA, proposée par Kornblith et al. (2019),
        est relativement simple et facile à manipuler dans la pratique.
      </p>

      <h4>Définition de CKA linéaire</h4>
      <p>
        Considérons les représentations de tokens d’une couche sous forme de matrices
        \(X \in \mathbb{R}^{N \times D}\) et
        \(Y \in \mathbb{R}^{N \times D}\),
        où \(N\) est le nombre de tokens et \(D\) la dimension cachée.
        En soustrayant la moyenne par ligne (sur la dimension des exemples), on obtient
        les matrices centrées \(\tilde{X}\) et \(\tilde{Y}\). On définit alors :
      </p>
      <p class="math-note">
        \[
          \mathrm{CKA}(X, Y)
          =
          \frac{
            \left\lVert \tilde{X}^\top \tilde{Y} \right\rVert_F^2
          }{
            \left\lVert \tilde{X}^\top \tilde{X} \right\rVert_F
            \cdot
            \left\lVert \tilde{Y}^\top \tilde{Y} \right\rVert_F
          }
        \]
      </p>
      <p>
        Ici, \(\lVert \cdot \rVert_F\) désigne la norme de Frobenius.
        La valeur est normalisée dans l’intervalle \(0 \leq \mathrm{CKA} \leq 1\),
        et <strong>plus elle est proche de 1, plus les deux espaces de représentation sont similaires</strong>.
      </p>

      <p>
        Dans TRepLiNa, on considère la matrice de représentation de la langue à faibles ressources
        comme \(X\), et celle de la langue à fortes ressources (langue pivot) comme \(Y\).
        Comme on souhaite que <strong>« les représentations des deux langues deviennent similaires »</strong>,
        on minimise \(1 - \mathrm{CKA}(X, Y)\) en tant que terme de perte :
      </p>
      <p class="math-note">
        \[
          \mathcal{L}_{\mathrm{CKA}} = 1 - \mathrm{CKA}(X, Y)
        \]
      </p>
      <p>
        Cependant, si l’on utilise uniquement CKA, il existe un risque que les représentations
        internes de la langue à fortes ressources – dont les « chemins » internes sont déjà bien
        organisés – soient entraînées vers l’espace de représentation de la langue à faibles ressources,
        dont les chemins internes ne sont pas encore bien structurés. Cela peut endommager
        l’organisation interne du modèle. L’article montre que l’utilisation conjointe de CKA
        et de REPINA permet d’obtenir un effet plus stable et plus efficace.
      </p>
    </section>

    <!-- Détails d’implémentation -->
    <section>
      <h2>Détails d’implémentation (CKA / REPINA / perte totale)</h2>

      <p>
        Le code complet est disponible sur GitHub :
        <a href="https://github.com/konta3738/cka-repina-aya23" target="_blank" rel="noopener noreferrer">
          https://github.com/konta3738/cka-repina-aya23
        </a><br />
        Dans ce qui suit, nous n’extrayons que les parties du code directement liées
        aux <strong>composants CKA, REPINA et à la fonction de perte totale</strong>.
      </p>

      <!-- Implémentation de CKA -->
      <h3>1. Partie implémentant CKA</h3>

      <pre><code>def center_rows(x: torch.Tensor, mask: Optional[torch.Tensor] = None) -&gt; torch.Tensor:
    """
    Center features along the sample dimension (rows).

    x: [N, D]
    mask: [N] boolean or float mask (1 for keep). If provided, compute mean over masked rows.
    """
    if mask is None:
        mean = x.mean(dim=0, keepdim=True)
        return x - mean
    else:
        if mask.dtype != torch.float32:
            mask = mask.float()
        # avoid division by zero
        denom = mask.sum().clamp(min=1.0)
        mean = (x * mask.unsqueeze(1)).sum(dim=0, keepdim=True) / denom
        return x - mean</code></pre>

      <pre><code>def linear_cka(x: torch.Tensor, y: torch.Tensor,
               x_mask: Optional[torch.Tensor] = None,
               y_mask: Optional[torch.Tensor] = None,
               eps: float = 1e-6) -&gt; torch.Tensor:
    """
    Compute linear CKA between two representation matrices.

    x: [N, D]
    y: [M, D]  (we'll downsample/upsample to min(N,M) by truncation to align tokens if not equal)
    masks: optional [N] and [M] masks
    """
    # If different number of rows (tokens), align by truncation to min length.
    n = x.shape[0]
    m = y.shape[0]
    L = min(n, m)
    x = x[:L]
    y = y[:L]
    if x_mask is not None:
        x_mask = x_mask[:L]
    if y_mask is not None:
        y_mask = y_mask[:L]

    # Center rows
    x = center_rows(x, x_mask)
    y = center_rows(y, y_mask)

    # Following Kornblith et al. (2019): CKA = ||X^T Y||_F^2 / (||X^T X||_F * ||Y^T Y||_F)
    # where X, Y are row-centered.
    xty = x.T @ y
    num = (xty * xty).sum()

    xtx = x.T @ x
    yty = y.T @ y
    denom = torch.sqrt((xtx * xtx).sum().clamp(min=eps)) * \
            torch.sqrt((yty * yty).sum().clamp(min=eps))
    return (num / denom).clamp(min=0.0, max=1.0)</code></pre>

      <!-- Implémentation de REPINA -->
      <h3>2. Partie implémentant REPINA<sup>I</sup></h3>

      <pre><code># 3) REPINA^I (identity projection): L2 between base (adapters disabled)
# and current reps, same inputs as a_align
do_repina = ((global_step + 1) % 2 == 0)  # every 2 steps
if do_repina:
    with torch.no_grad():
        with model.disable_adapter():
            base_out = forward_hidden(model, a_ids, a_attn)

    base_reps_all, _ = gather_layer_hidden(base_out.hidden_states, layer_index, a_attn)
    rep_cur = a_reps_all[a_src_keep]
    rep_pre = base_reps_all[a_src_keep]
    if rep_cur.shape[0] != rep_pre.shape[0]:
        L = min(rep_cur.shape[0], rep_pre.shape[0])
        rep_cur = rep_cur[:L]
        rep_pre = rep_pre[:L]

    repina_loss = torch.mean((rep_cur - rep_pre) ** 2)</code></pre>

      <!-- Perte totale -->
      <h3>3. Fonction de perte totale</h3>

      <pre><code>def compute_losses(
    model,
    batch: Dict[str, torch.Tensor],
    tokenizer,
    layer_index: int,
    lambda_cka: float,
    mu_repina: float,
    bf16: bool,
    global_step
):
    """
    Returns (total_loss, dict of components)
    """
    device = next(model.parameters()).device
    repina_loss = torch.tensor(0.0, device=device)

    # 1) Task loss: LM on prompt+target (labels already mask prompt with -100)
    input_ids = batch["input_ids"].to(device)
    labels    = batch["labels"].to(device)
    attention_mask = (input_ids != tokenizer.pad_token_id).to(device)

    outputs = model(
        input_ids=input_ids,
        attention_mask=attention_mask,
        labels=labels,
        use_cache=False,
        output_hidden_states=True
    )
    task_loss = compute_task_loss_with_label_smoothing(
        outputs, labels, epsilon=0.1, ignore_index=-100
    )

    # 2) Alignment loss (CKA) on source regions of A and B
    a_ids = batch["a_align_ids"].to(device)
    b_ids = batch["b_align_ids"].to(device)

    a_attn = (a_ids != tokenizer.pad_token_id).to(device)
    b_attn = (b_ids != tokenizer.pad_token_id).to(device)

    a_out = forward_hidden(model, a_ids, a_attn)
    b_out = forward_hidden(model, b_ids, b_attn)

    a_reps_all, a_tokmask_all = gather_layer_hidden(a_out.hidden_states, layer_index, a_attn)
    b_reps_all, b_tokmask_all = gather_layer_hidden(b_out.hidden_states, layer_index, b_attn)

    a_src_keep = a_attn.reshape(-1).bool()
    b_src_keep = b_attn.reshape(-1).bool()

    a_src = a_reps_all[a_src_keep]
    b_src = b_reps_all[b_src_keep]

    if a_src.numel() == 0:
        a_src = a_reps_all[a_tokmask_all]
    if b_src.numel() == 0:
        b_src = b_reps_all[b_tokmask_all]

    cka_val = linear_cka(a_src, b_src, None, None)
    cka_loss = 1.0 - cka_val

    # 3) REPINA^I
    do_repina = ((global_step + 1) % 2 == 0)
    if do_repina:
        with torch.no_grad():
            with model.disable_adapter():
                base_out = forward_hidden(model, a_ids, a_attn)
        base_reps_all, _ = gather_layer_hidden(base_out.hidden_states, layer_index, a_attn)
        rep_cur = a_reps_all[a_src_keep]
        rep_pre = base_reps_all[a_src_keep]
        if rep_cur.shape[0] != rep_pre.shape[0]:
            L = min(rep_cur.shape[0], rep_pre.shape[0])
            rep_cur = rep_cur[:L]
            rep_pre = rep_pre[:L]

        repina_loss = torch.mean((rep_cur - rep_pre) ** 2)

        total = task_loss + lambda_cka * cka_loss + mu_repina * repina_loss
    else:
        total = task_loss + lambda_cka * cka_loss

    return total, {
        "task_loss": task_loss.detach().float().item(),
        "cka": cka_val.detach().float().item(),
        "cka_loss": cka_loss.detach().float().item(),
        "repina": repina_loss.detach().float().item(),
        "total": total.detach().float().item()
    }</code></pre>

      <h4>Résumé sous forme de formules</h4>
      <p class="math-note">
        Si l’on note la perte de tâche \(\mathcal{L}_{\mathrm{task}}\),
        la perte CKA \(\mathcal{L}_{\mathrm{CKA}} = 1 - \mathrm{CKA}(X,Y)\),
        et la perte REPINA<sup>I</sup> \(\mathcal{L}_{\mathrm{REPINA}^{I}}\),
        alors la perte totale s’écrit :
        \[
          \mathcal{L}_{\mathrm{total}}
          =
          \mathcal{L}_{\mathrm{task}}
          + \lambda_{\mathrm{CKA}} \cdot \mathcal{L}_{\mathrm{CKA}}
          + \mu_{\mathrm{REPINA}} \cdot \mathcal{L}_{\mathrm{REPINA}^{I}}
        \]
        (dans l’implémentation, le terme REPINA n’est ajouté qu’une fois tous les quelques pas d’optimisation).
      </p>
    </section>
  </main>

  <footer>
    Page explicative (version HTML) : TRepLiNa / réglage CKA + REPINA
  </footer>
</body>
</html>
