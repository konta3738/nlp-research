<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>TRepLiNa: Improving Low-Resource Machine Translation with CKA + REPINA</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <style>
    body {
      font-family: system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif;
      line-height: 1.7;
      margin: 0;
      padding: 0;
      background: #f7f7fb;
      color: #222;
    }
    header {
      background: #1b2430;
      color: #fff;
      padding: 1.5rem 1rem;
      display: flex;
      justify-content: space-between;
      align-items: center;
    }
    header h1 {
      margin: 0;
      font-size: 1.8rem;
    }
    .lang-switch {
      margin-right: 1rem;
    }
    .lang-switch a {
      background: #fff;
      color: #1b2430;
      padding: 6px 14px;
      border-radius: 8px;
      text-decoration: none;
      font-size: 0.9rem;
      font-weight: bold;
    }
    .lang-switch a:hover {
      background: #dce0ff;
    }
    main {
      max-width: 900px;
      margin: 0 auto;
      padding: 1.5rem 1rem 3rem;
      background: #fff;
    }
    h2, h3, h4 {
      margin-top: 2rem;
      color: #1b2430;
    }
    h2 {
      border-bottom: 2px solid #e0e0f0;
      padding-bottom: 0.3rem;
    }
    a {
      color: #0066cc;
      text-decoration: none;
    }
    a:hover {
      text-decoration: underline;
    }
    .paper-info {
      background: #f0f2ff;
      border-left: 4px solid #4c6fff;
      padding: 0.8rem 1rem;
      margin: 1rem 0;
      font-size: 0.95rem;
    }
    .math-note {
      font-size: 0.9rem;
      color: #555;
      background: #fafafa;
      border-left: 3px solid #ccc;
      padding: 0.6rem 0.8rem;
      margin-top: 0.5rem;
    }
    pre {
      background: #0f172a;
      color: #e5e7eb;
      padding: 1rem;
      overflow-x: auto;
      border-radius: 6px;
      font-size: 0.85rem;
    }
  </style>

  <!-- MathJax -->
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [["\\(", "\\)"], ["$", "$"]],
        displayMath: [["\\[", "\\]"], ["$$", "$$"]]
      }
    };
  </script>
  <script async id="MathJax-script"
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>
  <header>
    <div>
      <h1>TRepLiNa: Improving Low-Resource Machine Translation with CKA + REPINA Alignment</h1>
      <p>Layer-wise representation alignment–based low-resource language tuning for Aya-23 8B</p>
    </div>

    <!-- Language switch -->
    <div class="lang-switch" id="lang-switch"></div>
  </header>

  <main>
    <section>
      <div class="paper-info">
        <span class="tag">Paper</span>
        <strong>TRepLiNa: Layer-wise CKA+REPINA Alignment Improves Low-Resource Machine Translation in Aya-23 8B</strong><br />
        arXiv: <a href="https://arxiv.org/abs/2510.06249" target="_blank" rel="noopener noreferrer">
          https://arxiv.org/abs/2510.06249
        </a><br />
        Publication year: 2025
      </div>

      <p>
        On this page, I would like to walk through the TRepLiNa paper. This work proposes a method that applies
        insights from <strong>mechanistic interpretability</strong>—the field that analyzes internal
        representations and neuron behavior in language models—to improve machine translation performance
        for low-resource languages.
      </p>

      <p>
        Research in mechanistic interpretability suggests, for example, that many large language models
        “take various languages as input but internally map them into a representation space close to English
        before doing further processing.” Put more informally, we are in a situation where
        it is reasonable to say that <strong>“language models think in English on the inside.”</strong>
      </p>

      <p>
        When the model is learning a low-resource language that it has almost never seen before,
        the pathway from that language to the “internal representations used to think in English”
        is not yet well organized. TRepLiNa focuses on this point and, by including
        the <strong>similarity between the internal representations of a newly seen language and a
        high-resource language (mainly English in this work)</strong> in the loss function,
        aims to better organize the path the model uses to “think in English” for low-resource languages.
      </p>

      <p>
        As a result, on low-resource language MT tasks, the paper shows cases where this method achieves
        higher BLEU / chrF++ scores than standard fine-tuning
        (although the strength of the effect also depends on factors such as cross-linguistic similarity).
      </p>

      <p>
        The experiments in the paper mainly focus on the
        <strong>low-resource language → high-resource language</strong> direction (e.g., LRL → English),
        but the results also suggest that the method can be beneficial in the
        <strong>high-resource language → low-resource language</strong> direction.
      </p>
    </section>

    <!-- Mechanism -->
    <section>
      <h2>High-level mechanism</h2>
      <p>
        In TRepLiNa, to control the <strong>similarity of internal representations</strong>
        between a low-resource language and a high-resource language, the loss function combines
        the following two techniques:
      </p>
      <ul>
        <li><strong>REPINA</strong>: a regularizer that suppresses representation drift between the base model and the fine-tuned model</li>
        <li><strong>CKA (Centered Kernel Alignment)</strong>: a metric that measures the similarity between representation spaces of different languages</li>
      </ul>

      <!-- REPINA -->
      <h3>How REPINA works</h3>
      <p>
        REPINA was originally proposed as a
        <strong>representation-preserving regularizer</strong> that controls
        “how much the representations after fine-tuning deviate from those of the base model.”
      </p>

      <h4>General intuition (REPINA with a linear projection)</h4>
      <p>
        For a token \(i\) at layer \(\ell\), we define:
      </p>
      <ul>
        <li>\(h_{\ell,i}^{(0)}\): hidden state before fine-tuning (base model)</li>
        <li>\(h_{\ell,i}^{(\theta)}\): hidden state after fine-tuning (model with parameters \(\theta\))</li>
      </ul>
      <p>
        REPINA assumes that there exists a linear mapping \(W\) between the base model’s representation space
        and the fine-tuned model’s representation space, and learns \(W\) so that
        \(W h_{\ell,i}^{(\theta)}\) becomes as close as possible to \(h_{\ell,i}^{(0)}\).
      </p>

      <p>
        The REPINA loss can typically be written as:
      </p>
      <p class="math-note">
        \[
          \mathcal{L}_{\mathrm{REPINA}}
          = \frac{1}{N}
            \sum_{i=1}^{N}
              \left\lVert
                W h_{\ell,i}^{(\theta)} - h_{\ell,i}^{(0)}
              \right\rVert_2^2
        \]
      </p>

      <h4>REPINA<sup>I</sup> in this implementation (identity projection)</h4>
      <p>
        In TRepLiNa’s implementation, a simpler variant,
        <strong>REPINA<sup>I</sup> (identity projection version)</strong>,
        is used. Here we treat \(W = I\) (the identity matrix). In that case, the loss becomes:
      </p>
      <p class="math-note">
        \[
          \mathcal{L}_{\mathrm{REPINA}^{I}}
          = \frac{1}{N}
            \sum_{i=1}^{N}
              \left\lVert
                h_{\ell,i}^{(\theta)} - h_{\ell,i}^{(0)}
              \right\rVert_2^2
        \]
      </p>
      <p>
        In other words, we penalize, using an L2 distance,
        situations where the <strong>representations obtained after fine-tuning with LoRA, etc.</strong>
        drift too far away from the <strong>representations of the base Aya-23 model with adapters disabled</strong>.
      </p>

      <!-- CKA -->
      <h3>How CKA works</h3>
      <p>
        CKA (Centered Kernel Alignment) is a metric that measures
        <strong>how similar the “shape” or “structure” of two representation matrices is</strong>.
        Linear CKA, in particular, is a relatively simple and easy-to-use variant
        proposed by Kornblith et al. (2019).
      </p>

      <h4>Definition of linear CKA</h4>
      <p>
        Let us consider the token representations of a layer as matrices
        \(X \in \mathbb{R}^{N \times D}\) and
        \(Y \in \mathbb{R}^{N \times D}\)
        (where \(N\) is the number of tokens and \(D\) is the hidden dimension).
        After subtracting the mean across rows (sample dimension),
        we obtain centered matrices \(\tilde{X}\) and \(\tilde{Y}\). Then:
      </p>
      <p class="math-note">
        \[
          \mathrm{CKA}(X, Y)
          =
          \frac{
            \left\lVert \tilde{X}^\top \tilde{Y} \right\rVert_F^2
          }{
            \left\lVert \tilde{X}^\top \tilde{X} \right\rVert_F
            \cdot
            \left\lVert \tilde{Y}^\top \tilde{Y} \right\rVert_F
          }
        \]
      </p>
      <p>
        Here, \(\lVert \cdot \rVert_F\) denotes the Frobenius norm.
        The value is normalized to \(0 \leq \mathrm{CKA} \leq 1\),
        and <strong>the closer it is to 1, the more similar the two representation spaces are</strong>.
      </p>

      <p>
        In TRepLiNa, we treat the representation matrix of the low-resource language as \(X\),
        and that of the high-resource language (pivot language) as \(Y\), and compute CKA between them.
        Since we want <strong>“the representations of the two languages to be similar,”</strong>
        we minimize \(1 - \mathrm{CKA}(X, Y)\) as a loss term:
      </p>
      <p class="math-note">
        \[
          \mathcal{L}_{\mathrm{CKA}} = 1 - \mathrm{CKA}(X, Y)
        \]
      </p>
      <p>
        However, if we use only CKA, there is a risk that the internal representations of the
        high-resource language—whose internal “pathways” are already well organized—will be
        dragged towards the representation space of the low-resource language, whose internal
        pathways are not yet well structured. This may damage the internal pathways of the model.
        The paper reports that using CKA together with REPINA leads to more effective and stable improvements.
      </p>
    </section>

    <!-- Implementation details -->
    <section>
      <h2>Implementation details (CKA / REPINA / total loss)</h2>

      <p>
        The full code is available on GitHub:
        <a href="https://github.com/konta3738/cka-repina-aya23" target="_blank" rel="noopener noreferrer">
          https://github.com/konta3738/cka-repina-aya23
        </a><br />
        Below, I extract and explain only the parts of the provided code that are related to the
        <strong>CKA component, REPINA component, and the overall loss function</strong>.
      </p>

      <!-- CKA implementation -->
      <h3>1. CKA implementation</h3>

      <pre><code>def center_rows(x: torch.Tensor, mask: Optional[torch.Tensor] = None) -&gt; torch.Tensor:
    """
    Center features along the sample dimension (rows).

    x: [N, D]
    mask: [N] boolean or float mask (1 for keep). If provided, compute mean over masked rows.
    """
    if mask is None:
        mean = x.mean(dim=0, keepdim=True)
        return x - mean
    else:
        if mask.dtype != torch.float32:
            mask = mask.float()
        # avoid division by zero
        denom = mask.sum().clamp(min=1.0)
        mean = (x * mask.unsqueeze(1)).sum(dim=0, keepdim=True) / denom
        return x - mean</code></pre>

      <pre><code>def linear_cka(x: torch.Tensor, y: torch.Tensor,
               x_mask: Optional[torch.Tensor] = None,
               y_mask: Optional[torch.Tensor] = None,
               eps: float = 1e-6) -&gt; torch.Tensor:
    """
    Compute linear CKA between two representation matrices.

    x: [N, D]
    y: [M, D]  (we'll downsample/upsample to min(N,M) by truncation to align tokens if not equal)
    masks: optional [N] and [M] masks
    """
    # If different number of rows (tokens), align by truncation to min length.
    n = x.shape[0]
    m = y.shape[0]
    L = min(n, m)
    x = x[:L]
    y = y[:L]
    if x_mask is not None:
        x_mask = x_mask[:L]
    if y_mask is not None:
        y_mask = y_mask[:L]

    # Center rows
    x = center_rows(x, x_mask)
    y = center_rows(y, y_mask)

    # Following Kornblith et al. (2019): CKA = ||X^T Y||_F^2 / (||X^T X||_F * ||Y^T Y||_F)
    # where X, Y are row-centered.
    xty = x.T @ y
    num = (xty * xty).sum()

    xtx = x.T @ x
    yty = y.T @ y
    denom = torch.sqrt((xtx * xtx).sum().clamp(min=eps)) * \
            torch.sqrt((yty * yty).sum().clamp(min=eps))
    return (num / denom).clamp(min=0.0, max=1.0)</code></pre>

      <!-- REPINA implementation -->
      <h3>2. REPINA<sup>I</sup> implementation</h3>

      <pre><code># 3) REPINA^I (identity projection): L2 between base (adapters disabled)
# and current reps, same inputs as a_align
do_repina = ((global_step + 1) % 2 == 0)  # every 2 steps
if do_repina:
    with torch.no_grad():
        with model.disable_adapter():
            base_out = forward_hidden(model, a_ids, a_attn)

    base_reps_all, _ = gather_layer_hidden(base_out.hidden_states, layer_index, a_attn)
    rep_cur = a_reps_all[a_src_keep]
    rep_pre = base_reps_all[a_src_keep]
    if rep_cur.shape[0] != rep_pre.shape[0]:
        L = min(rep_cur.shape[0], rep_pre.shape[0])
        rep_cur = rep_cur[:L]
        rep_pre = rep_pre[:L]

    repina_loss = torch.mean((rep_cur - rep_pre) ** 2)</code></pre>

      <!-- Overall loss -->
      <h3>3. Overall loss function</h3>

      <pre><code>def compute_losses(
    model,
    batch: Dict[str, torch.Tensor],
    tokenizer,
    layer_index: int,
    lambda_cka: float,
    mu_repina: float,
    bf16: bool,
    global_step
):
    """
    Returns (total_loss, dict of components)
    """
    device = next(model.parameters()).device
    repina_loss = torch.tensor(0.0, device=device)

    # 1) Task loss: LM on prompt+target (labels already mask prompt with -100)
    input_ids = batch["input_ids"].to(device)
    labels    = batch["labels"].to(device)
    attention_mask = (input_ids != tokenizer.pad_token_id).to(device)

    outputs = model(
        input_ids=input_ids,
        attention_mask=attention_mask,
        labels=labels,
        use_cache=False,
        output_hidden_states=True
    )
    task_loss = compute_task_loss_with_label_smoothing(
        outputs, labels, epsilon=0.1, ignore_index=-100
    )

    # 2) Alignment loss (CKA) on source regions of A and B
    a_ids = batch["a_align_ids"].to(device)
    b_ids = batch["b_align_ids"].to(device)

    a_attn = (a_ids != tokenizer.pad_token_id).to(device)
    b_attn = (b_ids != tokenizer.pad_token_id).to(device)

    a_out = forward_hidden(model, a_ids, a_attn)
    b_out = forward_hidden(model, b_ids, b_attn)

    a_reps_all, a_tokmask_all = gather_layer_hidden(a_out.hidden_states, layer_index, a_attn)
    b_reps_all, b_tokmask_all = gather_layer_hidden(b_out.hidden_states, layer_index, b_attn)

    a_src_keep = a_attn.reshape(-1).bool()
    b_src_keep = b_attn.reshape(-1).bool()

    a_src = a_reps_all[a_src_keep]
    b_src = b_reps_all[b_src_keep]

    if a_src.numel() == 0:
        a_src = a_reps_all[a_tokmask_all]
    if b_src.numel() == 0:
        b_src = b_reps_all[b_tokmask_all]

    cka_val = linear_cka(a_src, b_src, None, None)
    cka_loss = 1.0 - cka_val

    # 3) REPINA^I
    do_repina = ((global_step + 1) % 2 == 0)
    if do_repina:
        with torch.no_grad():
            with model.disable_adapter():
                base_out = forward_hidden(model, a_ids, a_attn)
        base_reps_all, _ = gather_layer_hidden(base_out.hidden_states, layer_index, a_attn)
        rep_cur = a_reps_all[a_src_keep]
        rep_pre = base_reps_all[a_src_keep]
        if rep_cur.shape[0] != rep_pre.shape[0]:
            L = min(rep_cur.shape[0], rep_pre.shape[0])
            rep_cur = rep_cur[:L]
            rep_pre = rep_pre[:L]

        repina_loss = torch.mean((rep_cur - rep_pre) ** 2)

        total = task_loss + lambda_cka * cka_loss + mu_repina * repina_loss
    else:
        total = task_loss + lambda_cka * cka_loss

    return total, {
        "task_loss": task_loss.detach().float().item(),
        "cka": cka_val.detach().float().item(),
        "cka_loss": cka_loss.detach().float().item(),
        "repina": repina_loss.detach().float().item(),
        "total": total.detach().float().item()
    }</code></pre>

      <h4>Summary in equation form</h4>
      <p class="math-note">
        Let \(\mathcal{L}_{\mathrm{task}}\) be the task loss,
        \(\mathcal{L}_{\mathrm{CKA}} = 1 - \mathrm{CKA}(X,Y)\) the CKA loss,
        and \(\mathcal{L}_{\mathrm{REPINA}^{I}}\) the REPINA<sup>I</sup> loss. Then the total loss is:
        \[
          \mathcal{L}_{\mathrm{total}}
          =
          \mathcal{L}_{\mathrm{task}}
          + \lambda_{\mathrm{CKA}} \cdot \mathcal{L}_{\mathrm{CKA}}
          + \mu_{\mathrm{REPINA}} \cdot \mathcal{L}_{\mathrm{REPINA}^{I}}
        \]
        (in the implementation, the REPINA term is only added every fixed number of steps).
      </p>
    </section>
  </main>

  <footer>
    TRepLiNa / CKA + REPINA tuning explanation page (HTML version)
  </footer>
<script src="lang-switch.js"></script>
</body>
</html>
