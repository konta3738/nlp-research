<!DOCTYPE html>
<html lang="da">
<head>
  <meta charset="UTF-8" />
  <title>TRepLiNa: Forbedring af maskinoversættelse for lavressourcesprog med CKA + REPINA</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <style>
    body {
      font-family: system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif;
      line-height: 1.7;
      margin: 0;
      padding: 0;
      background: #f7f7fb;
      color: #222;
    }
    header {
      background: #1b2430;
      color: #fff;
      padding: 1.5rem 1rem;
      display: flex;
      justify-content: space-between;
      align-items: center;
    }
    header h1 {
      margin: 0;
      font-size: 1.8rem;
    }
    .lang-switch {
      margin-right: 1rem;
    }
    .lang-switch a {
      background: #fff;
      color: #1b2430;
      padding: 6px 14px;
      border-radius: 8px;
      text-decoration: none;
      font-size: 0.9rem;
      font-weight: bold;
    }
    .lang-switch a:hover {
      background: #dce0ff;
    }
    main {
      max-width: 900px;
      margin: 0 auto;
      padding: 1.5rem 1rem 3rem;
      background: #fff;
    }
    h2, h3, h4 {
      margin-top: 2rem;
      color: #1b2430;
    }
    h2 {
      border-bottom: 2px solid #e0e0f0;
      padding-bottom: 0.3rem;
    }
    a {
      color: #0066cc;
      text-decoration: none;
    }
    a:hover {
      text-decoration: underline;
    }
    .paper-info {
      background: #f0f2ff;
      border-left: 4px solid #4c6fff;
      padding: 0.8rem 1rem;
      margin: 1rem 0;
      font-size: 0.95rem;
    }
    .math-note {
      font-size: 0.9rem;
      color: #555;
      background: #fafafa;
      border-left: 3px solid #ccc;
      padding: 0.6rem 0.8rem;
      margin-top: 0.5rem;
    }
    pre {
      background: #0f172a;
      color: #e5e7eb;
      padding: 1rem;
      overflow-x: auto;
      border-radius: 6px;
      font-size: 0.85rem;
    }
  </style>

  <!-- MathJax -->
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [["\\(", "\\)"], ["$", "$"]],
        displayMath: [["\\[", "\\]"], ["$$", "$$"]]
      }
    };
  </script>
  <script async id="MathJax-script"
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>
  <header>
    <div>
      <h1>TRepLiNa: forbedrer maskinoversættelse for lavressourcesprog med CKA + REPINA-alignment</h1>
      <p>Finjustering af Aya-23 8B til lavressourcesprog baseret på lagvis repræsentations-alignment</p>
    </div>

    <!-- Sprogskift -->
    <div class="lang-switch" id="lang-switch"></div>
  </header>

  <main>
    <section>
      <div class="paper-info">
        <span class="tag">Artikel</span>
        <strong>TRepLiNa: Layer-wise CKA+REPINA Alignment Improves Low-Resource Machine Translation in Aya-23 8B</strong><br />
        arXiv: <a href="https://arxiv.org/abs/2510.06249" target="_blank" rel="noopener noreferrer">
          https://arxiv.org/abs/2510.06249
        </a><br />
        Udgivelsesår: 2025
      </div>

      <p>
        På denne side vil jeg gerne forklare TRepLiNa-artiklen. Artiklen foreslår en metode,
        som bygger på indsigter fra
        <strong>mechanistic interpretability (mekanistisk fortolkelighed)</strong>,
        et forskningsområde der analyserer sprogmodellers interne repræsentationer og neuroners funktion,
        og anvender disse indsigter til at forbedre maskinoversættelse for lavressourcesprog.
      </p>

      <p>
        Forskning i mechanistic interpretability har f.eks. antydet, at mange store sprogmodeller
        – selv om de kan tage imod en lang række forskellige sprog som input –
        internt først projicerer disse sprog ind i et repræsentationsrum, der ligner engelsk,
        og derefter bearbejder informationen dér. Groft sagt kan man sige, at
        <strong>”sprogmodellen i praksis tænker på engelsk indvendigt”</strong>.
      </p>

      <p>
        Når modellen skal lære et lavressourcesprog, som den næsten aldrig har set før,
        er stierne fra dette sprog til de interne repræsentationer, hvor modellen ”tænker på engelsk”,
        endnu ikke ordentligt organiserede. TRepLiNa fokuserer på netop dette punkt og
        inkluderer <strong>repræsentationsligheden mellem det nye (lavressource-)sprog
        og et højressourcesprog (her primært engelsk)</strong> i tabsfunktionen
        for at hjælpe modellen med at etablere en mere velstruktureret vej
        til disse interne engelsklignende repræsentationer.
      </p>

      <p>
        Som resultat viser artiklen, at modellen i maskinoversættelsesopgaver for lavressourcesprog
        i flere tilfælde kan opnå højere BLEU- og chrF++-scores end med almindelig finjustering
        (effekten afhænger dog også af faktorer som typologisk lighed mellem sprogene).
      </p>

      <p>
        Artiklen fokuserer hovedsageligt på oversættelsesretningen
        <strong>lavressourcesprog → højressourcesprog</strong> (f.eks. LRL → engelsk),
        men de eksperimentelle resultater indikerer også, at metoden kan have en positiv effekt
        i retningen <strong>højressourcesprog → lavressourcesprog</strong>.
      </p>
    </section>

    <!-- Mekanisme -->
    <section>
      <h2>Overblik over metoden</h2>
      <p>
        I TRepLiNa kontrolleres
        <strong>ligheden mellem de interne repræsentationer</strong> for lav- og højressourcesprog
        ved hjælp af en tabsfunktion, der kombinerer følgende to teknikker:
      </p>
      <ul>
        <li><strong>REPINA</strong>: en regularisering, der begrænser driften mellem basmodellens og den finjusterede models repræsentationer</li>
        <li><strong>CKA (Centered Kernel Alignment)</strong>: et mål for, hvor ens repræsentationsrummene er på tværs af forskellige sprog</li>
      </ul>

      <!-- REPINA -->
      <h3>Hvordan REPINA fungerer</h3>
      <p>
        REPINA er oprindeligt foreslået som en
        <strong>repræsentationsbevarende regularisering</strong>, der kontrollerer
        ”hvor meget repræsentationerne efter finjustering afviger fra basmodellens repræsentationer”.
      </p>

      <h4>Generel intuition (REPINA med lineær projektion)</h4>
      <p>
        For token \(i\) i lag \(\ell\) definerer vi:
      </p>
      <ul>
        <li>\(h_{\ell,i}^{(0)}\): skjult tilstand i basmodellen (før finjustering)</li>
        <li>\(h_{\ell,i}^{(\theta)}\): skjult tilstand efter finjustering (med parametre \(\theta\))</li>
      </ul>
      <p>
        REPINA antager, at der findes en lineær afbildning \(W\) mellem basmodellens
        repræsentationsrum og repræsentationsrummet i den finjusterede model,
        og lærer \(W\) så \(W h_{\ell,i}^{(\theta)}\) kommer så tæt som muligt på \(h_{\ell,i}^{(0)}\).
      </p>

      <p>
        REPINA-tabet kan typisk skrives som:
      </p>
      <p class="math-note">
        \[
          \mathcal{L}_{\mathrm{REPINA}}
          = \frac{1}{N}
            \sum_{i=1}^{N}
              \left\lVert
                W h_{\ell,i}^{(\theta)} - h_{\ell,i}^{(0)}
              \right\rVert_2^2
        \]
      </p>

      <h4>REPINA<sup>I</sup> i denne implementering (identitetsprojektion)</h4>
      <p>
        I TRepLiNa-implementeringen anvendes en enklere variant,
        <strong>REPINA<sup>I</sup> (identity projection)</strong>,
        hvor man sætter \(W = I\) (identitetsmatricen). Tabsfunktionen bliver da:
      </p>
      <p class="math-note">
        \[
          \mathcal{L}_{\mathrm{REPINA}^{I}}
          = \frac{1}{N}
            \sum_{i=1}^{N}
              \left\lVert
                h_{\ell,i}^{(\theta)} - h_{\ell,i}^{(0)}
              \right\rVert_2^2
        \]
      </p>
      <p>
        Med andre ord lægger vi en L2-straf på, så
        <strong>repræsentationerne efter finjustering, f.eks. med LoRA</strong>,
        ikke driver for langt væk fra
        <strong>bas-Aya-23-repræsentationerne med adapterne deaktiveret</strong>.
      </p>

      <!-- CKA -->
      <h3>Hvordan CKA fungerer</h3>
      <p>
        CKA (Centered Kernel Alignment) er et mål for
        <strong>hvor ens to repræsentationsmatricer er med hensyn til ”form” eller ”struktur”</strong>.
        Især er lineær CKA en relativt enkel og håndterbar variant, som blev foreslået af
        Kornblith et al. (2019).
      </p>

      <h4>Definition af lineær CKA</h4>
      <p>
        Lad tokenrepræsentationerne i et givet lag samles i matricerne
        \(X \in \mathbb{R}^{N \times D}\) og
        \(Y \in \mathbb{R}^{N \times D}\), hvor \(N\) er antal tokens og \(D\)
        er dimensionen af den skjulte repræsentation. Ved at trække middelværdien fra
        over rækkerne (over eksemplerne) får vi de centrerede matricer
        \(\tilde{X}\) og \(\tilde{Y}\):
      </p>
      <p class="math-note">
        \[
          \mathrm{CKA}(X, Y)
          =
          \frac{
            \left\lVert \tilde{X}^\top \tilde{Y} \right\rVert_F^2
          }{
            \left\lVert \tilde{X}^\top \tilde{X} \right\rVert_F
            \cdot
            \left\lVert \tilde{Y}^\top \tilde{Y} \right\rVert_F
          }
        \]
      </p>
      <p>
        Her er \(\lVert \cdot \rVert_F\) Frobenius-normen.
        Værdien normaliseres så \(0 \leq \mathrm{CKA} \leq 1\), og
        <strong>værdier tæt på 1 betyder, at de to repræsentationsrum ligner hinanden stærkt</strong>.
      </p>

      <p>
        I TRepLiNa bruges repræsentationsmatricen for lavressourcesproget som \(X\),
        og matricen for højressourcesproget (pivotsproget) som \(Y\), og CKA beregnes.
        Da vi ønsker, at
        <strong>”repræsentationerne for de to sprog skal ligne hinanden”</strong>,
        minimerer vi \(1 - \mathrm{CKA}(X,Y)\) som tab:
      </p>
      <p class="math-note">
        \[
          \mathcal{L}_{\mathrm{CKA}} = 1 - \mathrm{CKA}(X, Y)
        \]
      </p>
      <p>
        Der er dog en risiko: CKA kan trække de interne repræsentationer for højressourcesproget
        i retning af de endnu ikke velorganiserede repræsentationer for lavressourcesproget
        og dermed forstyrre de allerede veludbyggede interne stier i modellen.
        I artiklen rapporteres det, at CKA fungerer bedre i kombination med REPINA,
        fordi REPINA beskytter basmodellens repræsentationer og stabiliserer den interne struktur.
      </p>
    </section>

    <!-- Implementeringsdetaljer -->
    <section>
      <h2>Implementeringsdetaljer (CKA / REPINA / samlet tab)</h2>

      <p>
        Hele koden er tilgængelig på GitHub:
        <a href="https://github.com/konta3738/cka-repina-aya23" target="_blank" rel="noopener noreferrer">
          https://github.com/konta3738/cka-repina-aya23
        </a><br />
        Her udtrækker vi kun de dele, der direkte vedrører
        <strong>CKA-delen, REPINA-delen og den samlede tabsfunktion</strong>,
        baseret på den kode du har stillet til rådighed.
      </p>

      <!-- CKA-implementering -->
      <h3>1. Implementering af CKA</h3>

      <pre><code>def center_rows(x: torch.Tensor, mask: Optional[torch.Tensor] = None) -&gt; torch.Tensor:
    """
    Center features along the sample dimension (rows).

    x: [N, D]
    mask: [N] boolean or float mask (1 for keep). If provided, compute mean over masked rows.
    """
    if mask is None:
        mean = x.mean(dim=0, keepdim=True)
        return x - mean
    else:
        if mask.dtype != torch.float32:
            mask = mask.float()
        # avoid division by zero
        denom = mask.sum().clamp(min=1.0)
        mean = (x * mask.unsqueeze(1)).sum(dim=0, keepdim=True) / denom
        return x - mean

def linear_cka(x: torch.Tensor, y: torch.Tensor,
               x_mask: Optional[torch.Tensor] = None,
               y_mask: Optional[torch.Tensor] = None,
               eps: float = 1e-6) -&gt; torch.Tensor:
    """
    Compute linear CKA between two representation matrices.

    x: [N, D]
    y: [M, D]  (we'll downsample/upsample to min(N,M) by truncation to align tokens if not equal)
    masks: optional [N] and [M] masks
    """
    # If different number of rows (tokens), align by truncation to min length.
    n = x.shape[0]
    m = y.shape[0]
    L = min(n, m)
    x = x[:L]
    y = y[:L]
    if x_mask is not None:
        x_mask = x_mask[:L]
    if y_mask is not None:
        y_mask = y_mask[:L]

    # Center rows
    x = center_rows(x, x_mask)
    y = center_rows(y, y_mask)

    # Following Kornblith et al. (2019): CKA = ||X^T Y||_F^2 / (||X^T X||_F * ||Y^T Y||_F)
    # where X, Y are row-centered.
    xty = x.T @ y
    num = (xty * xty).sum()

    xtx = x.T @ x
    yty = y.T @ y
    denom = torch.sqrt((xtx * xtx).sum().clamp(min=eps)) * \
            torch.sqrt((yty * yty).sum().clamp(min=eps))
    return (num / denom).clamp(min=0.0, max=1.0)</code></pre>

      <!-- REPINA-implementering -->
      <h3>2. Implementering af REPINA<sup>I</sup></h3>

      <pre><code># 3) REPINA^I (identity projection): L2 between base (adapters disabled)
# and current reps, same inputs as a_align
do_repina = ((global_step + 1) % 2 == 0)  # every 2 steps
if do_repina:
    with torch.no_grad():
        with model.disable_adapter():
            base_out = forward_hidden(model, a_ids, a_attn)

    base_reps_all, _ = gather_layer_hidden(base_out.hidden_states, layer_index, a_attn)
    rep_cur = a_reps_all[a_src_keep]
    rep_pre = base_reps_all[a_src_keep]
    if rep_cur.shape[0] != rep_pre.shape[0]:
        L = min(rep_cur.shape[0], rep_pre.shape[0])
        rep_cur = rep_cur[:L]
        rep_pre = rep_pre[:L]

    repina_loss = torch.mean((rep_cur - rep_pre) ** 2)</code></pre>

      <!-- Samlet tabsfunktion -->
      <h3>3. Samlet tabsfunktion</h3>

      <pre><code>def compute_losses(
    model,
    batch: Dict[str, torch.Tensor],
    tokenizer,
    layer_index: int,
    lambda_cka: float,
    mu_repina: float,
    bf16: bool,
    global_step
):
    """
    Returns (total_loss, dict of components)
    """
    device = next(model.parameters()).device
    repina_loss = torch.tensor(0.0, device=device)

    # 1) Task loss: LM on prompt+target (labels already mask prompt with -100)
    input_ids = batch["input_ids"].to(device)
    labels    = batch["labels"].to(device)
    attention_mask = (input_ids != tokenizer.pad_token_id).to(device)

    outputs = model(
        input_ids=input_ids,
        attention_mask=attention_mask,
        labels=labels,
        use_cache=False,
        output_hidden_states=True
    )
    task_loss = compute_task_loss_with_label_smoothing(
        outputs, labels, epsilon=0.1, ignore_index=-100
    )

    # 2) Alignment loss (CKA) on source regions of A and B
    a_ids = batch["a_align_ids"].to(device)
    b_ids = batch["b_align_ids"].to(device)

    a_attn = (a_ids != tokenizer.pad_token_id).to(device)
    b_attn = (b_ids != tokenizer.pad_token_id).to(device)

    a_out = forward_hidden(model, a_ids, a_attn)
    b_out = forward_hidden(model, b_ids, b_attn)

    a_reps_all, a_tokmask_all = gather_layer_hidden(a_out.hidden_states, layer_index, a_attn)
    b_reps_all, b_tokmask_all = gather_layer_hidden(b_out.hidden_states, layer_index, b_attn)

    a_src_keep = a_attn.reshape(-1).bool()
    b_src_keep = b_attn.reshape(-1).bool()

    a_src = a_reps_all[a_src_keep]
    b_src = b_reps_all[b_src_keep]

    if a_src.numel() == 0:
        a_src = a_reps_all[a_tokmask_all]
    if b_src.numel() == 0:
        b_src = b_reps_all[b_tokmask_all]

    cka_val = linear_cka(a_src, b_src, None, None)
    cka_loss = 1.0 - cka_val

    # 3) REPINA^I
    do_repina = ((global_step + 1) % 2 == 0)
    if do_repina:
        with torch.no_grad():
            with model.disable_adapter():
                base_out = forward_hidden(model, a_ids, a_attn)
        base_reps_all, _ = gather_layer_hidden(base_out.hidden_states, layer_index, a_attn)
        rep_cur = a_reps_all[a_src_keep]
        rep_pre = base_reps_all[a_src_keep]
        if rep_cur.shape[0] != rep_pre.shape[0]:
            L = min(rep_cur.shape[0], rep_pre.shape[0])
            rep_cur = rep_cur[:L]
            rep_pre = rep_pre[:L]

        repina_loss = torch.mean((rep_cur - rep_pre) ** 2)

        total = task_loss + lambda_cka * cka_loss + mu_repina * repina_loss
    else:
        total = task_loss + lambda_cka * cka_loss

    return total, {
        "task_loss": task_loss.detach().float().item(),
        "cka": cka_val.detach().float().item(),
        "cka_loss": cka_loss.detach().float().item(),
        "repina": repina_loss.detach().float().item(),
        "total": total.detach().float().item()
    }</code></pre>

      <h4>Opsummering i form af formler</h4>
      <p class="math-note">
        Lader vi opgavetabet være \(\mathcal{L}_{\mathrm{task}}\),
        CKA-tabet \(\mathcal{L}_{\mathrm{CKA}} = 1 - \mathrm{CKA}(X,Y)\)
        og REPINA<sup>I</sup>-tabet \(\mathcal{L}_{\mathrm{REPINA}^{I}}\),
        kan den samlede tabsfunktion skrives som
        \[
          \mathcal{L}_{\mathrm{total}}
          =
          \mathcal{L}_{\mathrm{task}}
          + \lambda_{\mathrm{CKA}} \cdot \mathcal{L}_{\mathrm{CKA}}
          + \mu_{\mathrm{REPINA}} \cdot \mathcal{L}_{\mathrm{REPINA}^{I}}
        \]
        (i implementeringen tilføjes REPINA-delen kun med bestemte intervaller, f.eks.
        for hver anden træningsstep).
      </p>
    </section>
  </main>

  <footer>
    TRepLiNa / CKA + REPINA-tuning – forklaringsside (HTML-version)
  </footer>
<script src="lang-switch.js"></script>
</body>
</html>
