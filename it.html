<!DOCTYPE html>
<html lang="it">
<head>
  <meta charset="UTF-8" />
  <title>TRepLiNa: migliorare la traduzione automatica a basso numero di risorse con CKA + REPINA</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <style>
    body {
      font-family: system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif;
      line-height: 1.7;
      margin: 0;
      padding: 0;
      background: #f7f7fb;
      color: #222;
    }
    header {
      background: #1b2430;
      color: #fff;
      padding: 1.5rem 1rem;
      display: flex;
      justify-content: space-between;
      align-items: center;
    }
    header h1 {
      margin: 0;
      font-size: 1.8rem;
    }
    .lang-switch {
      margin-right: 1rem;
    }
    .lang-switch a {
      background: #fff;
      color: #1b2430;
      padding: 6px 14px;
      border-radius: 8px;
      text-decoration: none;
      font-size: 0.9rem;
      font-weight: bold;
    }
    .lang-switch a:hover {
      background: #dce0ff;
    }
    main {
      max-width: 900px;
      margin: 0 auto;
      padding: 1.5rem 1rem 3rem;
      background: #fff;
    }
    h2, h3, h4 {
      margin-top: 2rem;
      color: #1b2430;
    }
    h2 {
      border-bottom: 2px solid #e0e0f0;
      padding-bottom: 0.3rem;
    }
    a {
      color: #0066cc;
      text-decoration: none;
    }
    a:hover {
      text-decoration: underline;
    }
    .paper-info {
      background: #f0f2ff;
      border-left: 4px solid #4c6fff;
      padding: 0.8rem 1rem;
      margin: 1rem 0;
      font-size: 0.95rem;
    }
    .math-note {
      font-size: 0.9rem;
      color: #555;
      background: #fafafa;
      border-left: 3px solid #ccc;
      padding: 0.6rem 0.8rem;
      margin-top: 0.5rem;
    }
    pre {
      background: #0f172a;
      color: #e5e7eb;
      padding: 1rem;
      overflow-x: auto;
      border-radius: 6px;
      font-size: 0.85rem;
    }
  </style>

  <!-- MathJax -->
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [["\\(", "\\)"], ["$", "$"]],
        displayMath: [["\\[", "\\]"], ["$$", "$$"]]
      }
    };
  </script>
  <script async id="MathJax-script"
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>
  <header>
    <div>
      <h1>TRepLiNa: migliorare la traduzione automatica a basso numero di risorse con l’allineamento CKA + REPINA</h1>
      <p>Messa a punto di Aya-23 8B per lingue a bassa disponibilità di risorse basata su allineamento delle rappresentazioni per strato</p>
    </div>

    <!-- Selettore di lingua -->
    <div class="lang-switch" id="lang-switch"></div>
  </header>

  <main>
    <section>
      <div class="paper-info">
        <span class="tag">Articolo</span>
        <strong>TRepLiNa: Layer-wise CKA+REPINA Alignment Improves Low-Resource Machine Translation in Aya-23 8B</strong><br />
        arXiv: <a href="https://arxiv.org/abs/2510.06249" target="_blank" rel="noopener noreferrer">
          https://arxiv.org/abs/2510.06249
        </a><br />
        Anno di pubblicazione: 2025
      </div>

      <p>
        In questa pagina vorrei spiegare il contenuto dell’articolo TRepLiNa. Questo lavoro propone
        un metodo che applica, al miglioramento della traduzione automatica per lingue di poche risorse,
        le conoscenze derivate dal campo della
        <strong>mechanistic interpretability (interpretabilità meccanicistica)</strong>,
        cioè un’area di ricerca che analizza le rappresentazioni interne e il funzionamento dei neuroni
        nei modelli di linguaggio.
      </p>

      <p>
        Dai lavori in mechanistic interpretability emerge, ad esempio, che molti grandi modelli di linguaggio
        <strong>ricevono in input lingue diverse, ma internamente mappano queste lingue in uno spazio
        di rappresentazione vicino all’inglese prima di eseguire l’elaborazione</strong>.
        In termini più colloquiali, ci si trova nella situazione in cui si può dire che
        <strong>“il modello di linguaggio, internamente, pensa in inglese”</strong>.
      </p>

      <p>
        Quando il modello impara una lingua di poche risorse che ha praticamente visto per la prima volta,
        il percorso che collega quella lingua alle <strong>rappresentazioni interne con cui il modello
        “pensa in inglese”</strong> non è ancora ben organizzato.
        TRepLiNa si concentra proprio su questo punto e mira a organizzare meglio tale percorso
        includendo, nella funzione di perdita, la
        <strong>similarità tra le rappresentazioni della nuova lingua e quelle di una lingua di molte risorse
        (qui principalmente l’inglese)</strong>.
      </p>

      <p>
        Come risultato, nelle attività di traduzione automatica per lingue di poche risorse,
        l’articolo mostra casi in cui si ottengono punteggi BLEU / chrF++ più alti rispetto
        a un fine-tuning standard
        (sebbene l’effetto dipenda anche da fattori come la similarità tipologica tra le lingue).
      </p>

      <p>
        L’articolo si concentra soprattutto sulla direzione
        <strong>lingua di poche risorse → lingua di molte risorse</strong> (per esempio, LRL → inglese),
        ma dai risultati sperimentali si suggerisce che vi sia un effetto positivo anche nella direzione
        <strong>lingua di molte risorse → lingua di poche risorse</strong>.
      </p>
    </section>

    <!-- Meccanismo -->
    <section>
      <h2>Panoramica del meccanismo</h2>
      <p>
        In TRepLiNa, per controllare la <strong>similarità delle rappresentazioni interne</strong>
        tra una lingua di poche risorse e una lingua di molte risorse,
        si utilizza una funzione di perdita che combina le due tecniche seguenti:
      </p>
      <ul>
        <li><strong>REPINA</strong>: una regolarizzazione che limita la deriva delle rappresentazioni tra il modello base e il modello dopo il fine-tuning</li>
        <li><strong>CKA (Centered Kernel Alignment)</strong>: un indicatore che misura la similarità tra gli spazi di rappresentazione di lingue diverse</li>
      </ul>

      <!-- REPINA -->
      <h3>Come funziona REPINA</h3>
      <p>
        REPINA è stato originariamente proposto come una
        <strong>regolarizzazione orientata a preservare le rappresentazioni</strong>,
        che controlla <strong>quanto le rappresentazioni dopo il fine-tuning si discostano
        da quelle del modello base</strong>.
      </p>

      <h4>Immagine generale (REPINA con proiezione lineare)</h4>
      <p>
        Per un token \(i\) nello strato \(\ell\), definiamo:
      </p>
      <ul>
        <li>\(h_{\ell,i}^{(0)}\): stato nascosto prima del fine-tuning (modello base)</li>
        <li>\(h_{\ell,i}^{(\theta)}\): stato nascosto dopo il fine-tuning (modello con parametri \(\theta\))</li>
      </ul>
      <p>
        REPINA assume che esista una trasformazione lineare \(W\) tra lo spazio di rappresentazione
        del modello base e quello del modello fine-tunato, e apprende \(W\) in modo che
        \(W h_{\ell,i}^{(\theta)}\) si avvicini il più possibile a \(h_{\ell,i}^{(0)}\).
      </p>

      <p>
        La perdita REPINA può essere scritta, in forma tipica, come:
      </p>
      <p class="math-note">
        \[
          \mathcal{L}_{\mathrm{REPINA}}
          = \frac{1}{N}
            \sum_{i=1}^{N}
              \left\lVert
                W h_{\ell,i}^{(\theta)} - h_{\ell,i}^{(0)}
              \right\rVert_2^2
        \]
      </p>

      <h4>REPINA<sup>I</sup> in questa implementazione (proiezione identità)</h4>
      <p>
        Nell’implementazione di TRepLiNa si adotta una variante più semplice,
        <strong>REPINA<sup>I</sup> (versione con proiezione identità)</strong>, in cui si assume
        \(W = I\) (la matrice identità). In tal caso la perdita diventa:
      </p>
      <p class="math-note">
        \[
          \mathcal{L}_{\mathrm{REPINA}^{I}}
          = \frac{1}{N}
            \sum_{i=1}^{N}
              \left\lVert
                h_{\ell,i}^{(\theta)} - h_{\ell,i}^{(0)}
              \right\rVert_2^2
        \]
      </p>
      <p>
        In altre parole, si penalizzano tramite distanza L2 i casi in cui
        le <strong>rappresentazioni ottenute dopo il fine-tuning (ad esempio con LoRA)</strong>
        si allontanano troppo dalle <strong>rappresentazioni del modello Aya-23 base
        con gli adattatori disattivati</strong>.
      </p>

      <!-- CKA -->
      <h3>Come funziona CKA</h3>
      <p>
        CKA (Centered Kernel Alignment) è un indicatore che misura
        <strong>quanto la “forma” o la “struttura” di due matrici di rappresentazione
        siano simili</strong>. In particolare, la variante lineare proposta da
        Kornblith et al. (2019) è relativamente semplice e pratica da utilizzare.
      </p>

      <h4>Definizione di CKA lineare</h4>
      <p>
        Consideriamo le rappresentazioni dei token in uno strato come matrici
        \(X \in \mathbb{R}^{N \times D}\) e
        \(Y \in \mathbb{R}^{N \times D}\),
        dove \(N\) è il numero di token e \(D\) è la dimensione nascosta.
        Sottraendo la media per riga (lungo la dimensione dei campioni),
        otteniamo le versioni centrate \(\tilde{X}\) e \(\tilde{Y}\). Allora:
      </p>
      <p class="math-note">
        \[
          \mathrm{CKA}(X, Y)
          =
          \frac{
            \left\lVert \tilde{X}^\top \tilde{Y} \right\rVert_F^2
          }{
            \left\lVert \tilde{X}^\top \tilde{X} \right\rVert_F
            \cdot
            \left\lVert \tilde{Y}^\top \tilde{Y} \right\rVert_F
          }
        \]
      </p>
      <p>
        Qui \(\lVert \cdot \rVert_F\) è la norma di Frobenius.
        Il valore è normalizzato nell’intervallo \(0 \leq \mathrm{CKA} \leq 1\), e
        <strong>più è vicino a 1, più i due spazi di rappresentazione sono simili</strong>.
      </p>

      <p>
        In TRepLiNa, la matrice di rappresentazioni della lingua di poche risorse
        è presa come \(X\), mentre la matrice della lingua di molte risorse (lingua pivot)
        è presa come \(Y\). Poiché vogliamo che
        <strong>“le rappresentazioni delle due lingue siano simili”</strong>,
        utilizziamo \(1 - \mathrm{CKA}(X,Y)\) come termine di perdita da minimizzare:
      </p>
      <p class="math-note">
        \[
          \mathcal{L}_{\mathrm{CKA}} = 1 - \mathrm{CKA}(X, Y)
        \]
      </p>
      <p>
        Tuttavia, usando solo CKA, c’è il rischio che le rappresentazioni interne
        della lingua di molte risorse — i cui “percorsi” interni sono già ben organizzati —
        vengano attirate verso lo spazio di rappresentazione della lingua di poche risorse,
        i cui percorsi interni non sono ancora strutturati in modo adeguato, con la possibilità
        di danneggiare l’organizzazione interna complessiva del modello.
        L’articolo riporta che combinando CKA con REPINA si ottiene un effetto più stabile
        ed efficace.
      </p>
    </section>

    <!-- Dettagli di implementazione -->
    <section>
      <h2>Dettagli di implementazione (CKA / REPINA / perdita totale)</h2>

      <p>
        Il codice completo è disponibile su GitHub:
        <a href="https://github.com/konta3738/cka-repina-aya23" target="_blank" rel="noopener noreferrer">
          https://github.com/konta3738/cka-repina-aya23
        </a><br />
        Qui estraiamo soltanto le parti del codice direttamente legate a
        <strong>CKA, REPINA e alla funzione di perdita complessiva</strong>.
      </p>

      <!-- Implementazione CKA -->
      <h3>1. Parte di implementazione di CKA</h3>

      <pre><code>def center_rows(x: torch.Tensor, mask: Optional[torch.Tensor] = None) -&gt; torch.Tensor:
    """
    Center features along the sample dimension (rows).

    x: [N, D]
    mask: [N] boolean or float mask (1 for keep). If provided, compute mean over masked rows.
    """
    if mask is None:
        mean = x.mean(dim=0, keepdim=True)
        return x - mean
    else:
        if mask.dtype != torch.float32:
            mask = mask.float()
        # avoid division by zero
        denom = mask.sum().clamp(min=1.0)
        mean = (x * mask.unsqueeze(1)).sum(dim=0, keepdim=True) / denom
        return x - mean

def linear_cka(x: torch.Tensor, y: torch.Tensor,
               x_mask: Optional[torch.Tensor] = None,
               y_mask: Optional[torch.Tensor] = None,
               eps: float = 1e-6) -&gt; torch.Tensor:
    """
    Compute linear CKA between two representation matrices.

    x: [N, D]
    y: [M, D]  (we'll downsample/upsample to min(N,M) by truncation to align tokens if not equal)
    masks: optional [N] and [M] masks
    """
    # If different number of rows (tokens), align by truncation to min length.
    n = x.shape[0]
    m = y.shape[0]
    L = min(n, m)
    x = x[:L]
    y = y[:L]
    if x_mask is not None:
        x_mask = x_mask[:L]
    if y_mask is not None:
        y_mask = y_mask[:L]

    # Center rows
    x = center_rows(x, x_mask)
    y = center_rows(y, y_mask)

    # Following Kornblith et al. (2019): CKA = ||X^T Y||_F^2 / (||X^T X||_F * ||Y^T Y||_F)
    # where X, Y are row-centered.
    xty = x.T @ y
    num = (xty * xty).sum()

    xtx = x.T @ x
    yty = y.T @ y
    denom = torch.sqrt((xtx * xtx).sum().clamp(min=eps)) * \
            torch.sqrt((yty * yty).sum().clamp(min=eps))
    return (num / denom).clamp(min=0.0, max=1.0)</code></pre>

      <!-- Implementazione REPINA -->
      <h3>2. Parte di implementazione di REPINA<sup>I</sup></h3>

      <pre><code># 3) REPINA^I (identity projection): L2 between base (adapters disabled)
# and current reps, same inputs as a_align
do_repina = ((global_step + 1) % 2 == 0)  # every 2 steps
if do_repina:
    with torch.no_grad():
        with model.disable_adapter():
            base_out = forward_hidden(model, a_ids, a_attn)

    base_reps_all, _ = gather_layer_hidden(base_out.hidden_states, layer_index, a_attn)
    rep_cur = a_reps_all[a_src_keep]
    rep_pre = base_reps_all[a_src_keep]
    if rep_cur.shape[0] != rep_pre.shape[0]:
        L = min(rep_cur.shape[0], rep_pre.shape[0])
        rep_cur = rep_cur[:L]
        rep_pre = rep_pre[:L]

    repina_loss = torch.mean((rep_cur - rep_pre) ** 2)</code></pre>

      <!-- Perdita totale -->
      <h3>3. Funzione di perdita totale</h3>

      <pre><code>def compute_losses(
    model,
    batch: Dict[str, torch.Tensor],
    tokenizer,
    layer_index: int,
    lambda_cka: float,
    mu_repina: float,
    bf16: bool,
    global_step
):
    """
    Returns (total_loss, dict of components)
    """
    device = next(model.parameters()).device
    repina_loss = torch.tensor(0.0, device=device)

    # 1) Task loss: LM on prompt+target (labels already mask prompt with -100)
    input_ids = batch["input_ids"].to(device)
    labels    = batch["labels"].to(device)
    attention_mask = (input_ids != tokenizer.pad_token_id).to(device)

    outputs = model(
        input_ids=input_ids,
        attention_mask=attention_mask,
        labels=labels,
        use_cache=False,
        output_hidden_states=True
    )
    task_loss = compute_task_loss_with_label_smoothing(
        outputs, labels, epsilon=0.1, ignore_index=-100
    )

    # 2) Alignment loss (CKA) on source regions of A and B
    a_ids = batch["a_align_ids"].to(device)
    b_ids = batch["b_align_ids"].to(device)

    a_attn = (a_ids != tokenizer.pad_token_id).to(device)
    b_attn = (b_ids != tokenizer.pad_token_id).to(device)

    a_out = forward_hidden(model, a_ids, a_attn)
    b_out = forward_hidden(model, b_ids, b_attn)

    a_reps_all, a_tokmask_all = gather_layer_hidden(a_out.hidden_states, layer_index, a_attn)
    b_reps_all, b_tokmask_all = gather_layer_hidden(b_out.hidden_states, layer_index, b_attn)

    a_src_keep = a_attn.reshape(-1).bool()
    b_src_keep = b_attn.reshape(-1).bool()

    a_src = a_reps_all[a_src_keep]
    b_src = b_reps_all[b_src_keep]

    if a_src.numel() == 0:
        a_src = a_reps_all[a_tokmask_all]
    if b_src.numel() == 0:
        b_src = b_reps_all[b_tokmask_all]

    cka_val = linear_cka(a_src, b_src, None, None)
    cka_loss = 1.0 - cka_val

    # 3) REPINA^I
    do_repina = ((global_step + 1) % 2 == 0)
    if do_repina:
        with torch.no_grad():
            with model.disable_adapter():
                base_out = forward_hidden(model, a_ids, a_attn)
        base_reps_all, _ = gather_layer_hidden(base_out.hidden_states, layer_index, a_attn)
        rep_cur = a_reps_all[a_src_keep]
        rep_pre = base_reps_all[a_src_keep]
        if rep_cur.shape[0] != rep_pre.shape[0]:
            L = min(rep_cur.shape[0], rep_pre.shape[0])
            rep_cur = rep_cur[:L]
            rep_pre = rep_pre[:L]

        repina_loss = torch.mean((rep_cur - rep_pre) ** 2)

        total = task_loss + lambda_cka * cka_loss + mu_repina * repina_loss
    else:
        total = task_loss + lambda_cka * cka_loss

    return total, {
        "task_loss": task_loss.detach().float().item(),
        "cka": cka_val.detach().float().item(),
        "cka_loss": cka_loss.detach().float().item(),
        "repina": repina_loss.detach().float().item(),
        "total": total.detach().float().item()
    }</code></pre>

      <h4>Riepilogo in forma matematica</h4>
      <p class="math-note">
        Indicando con \(\mathcal{L}_{\mathrm{task}}\) la perdita di task,
        con \(\mathcal{L}_{\mathrm{CKA}} = 1 - \mathrm{CKA}(X,Y)\) la perdita CKA
        e con \(\mathcal{L}_{\mathrm{REPINA}^{I}}\) la perdita REPINA<sup>I</sup>,
        la perdita totale è
        \[
          \mathcal{L}_{\mathrm{total}}
          =
          \mathcal{L}_{\mathrm{task}}
          + \lambda_{\mathrm{CKA}} \cdot \mathcal{L}_{\mathrm{CKA}}
          + \mu_{\mathrm{REPINA}} \cdot \mathcal{L}_{\mathrm{REPINA}^{I}}
        \]
        (nell’implementazione, il termine REPINA viene aggiunto solo ogni certo numero di step).
      </p>
    </section>
  </main>

  <footer>
    Pagina esplicativa TRepLiNa / tuning con CKA + REPINA (versione HTML)
  </footer>
<script src="lang-switch.js"></script>
</body>
</html>
