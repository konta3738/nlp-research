<!DOCTYPE html>
<html lang="ja">
<head>
  <meta charset="UTF-8" />
  <title>TRepLiNa: CKA + REPINA による低資源機械翻訳の改善</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <!-- 簡単なスタイル -->
  <style>
    body {
      font-family: system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif;
      line-height: 1.7;
      margin: 0;
      padding: 0;
      background: #f7f7fb;
      color: #222;
    }
    header {
      background: #1b2430;
      color: #fff;
      padding: 1.5rem 1rem;
    }
    header h1 {
      margin: 0;
      font-size: 1.8rem;
    }
    header p {
      margin: 0.2rem 0 0;
      font-size: 0.9rem;
      opacity: 0.8;
    }
    main {
      max-width: 900px;
      margin: 0 auto;
      padding: 1.5rem 1rem 3rem;
      background: #fff;
    }
    h2, h3, h4 {
      margin-top: 2rem;
      color: #1b2430;
    }
    h2 {
      border-bottom: 2px solid #e0e0f0;
      padding-bottom: 0.3rem;
    }
    a {
      color: #0066cc;
      text-decoration: none;
    }
    a:hover {
      text-decoration: underline;
    }
    .paper-info {
      background: #f0f2ff;
      border-left: 4px solid #4c6fff;
      padding: 0.8rem 1rem;
      margin: 1rem 0;
      font-size: 0.95rem;
    }
    .math-note {
      font-size: 0.9rem;
      color: #555;
      background: #fafafa;
      border-left: 3px solid #ccc;
      padding: 0.6rem 0.8rem;
      margin-top: 0.5rem;
    }
    pre {
      background: #0f172a;
      color: #e5e7eb;
      padding: 1rem;
      overflow-x: auto;
      border-radius: 6px;
      font-size: 0.85rem;
    }
    code {
      font-family: "Fira Code", ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
    }
    .tag {
      display: inline-block;
      background: #edf2ff;
      color: #364fc7;
      border-radius: 999px;
      padding: 0.1rem 0.6rem;
      font-size: 0.75rem;
      margin-right: 0.4rem;
    }
    footer {
      max-width: 900px;
      margin: 0 auto;
      padding: 1rem;
      font-size: 0.8rem;
      color: #666;
    }
  </style>

  <!-- MathJax（数式用） -->
  <script>
    window.MathJax = {
      tex: { inlineMath: [["\\(", "\\)"], ["$", "$"]] }
    };
  </script>
  <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
  <header>
    <h1>TRepLiNa: CKA + REPINA Alignment で低資源機械翻訳を改善する</h1>
    <p>Layer-wise 表現アライメントに基づく Aya-23 8B の低資源言語チューニング</p>
  </header>

  <main>
    <section>
      <div class="paper-info">
        <span class="tag">論文</span>
        <strong>TRepLiNa: Layer-wise CKA+REPINA Alignment Improves Low-Resource Machine Translation in Aya-23 8B</strong><br />
        arXiv: <a href="https://arxiv.org/abs/2510.06249" target="_blank" rel="noopener noreferrer">
          https://arxiv.org/abs/2510.06249
        </a><br />
        公開年: 2025年
      </div>

      <p>
        本日は、TRepLiNa の論文について解説したいと思います。この論文は、
        <strong>mechanistic interpretability（機械的解釈可能性）</strong>という、
        言語モデルの内部表現やニューロンの働きを分析する分野から得られた知見を、
        低資源言語での機械翻訳の向上に応用した手法を提案しています。
      </p>

      <p>
        mechanistic interpretability の研究から、例えば多くの大規模言語モデルは
        「さまざまな言語を入力として受け取りつつ、内部では英語に近い表現空間にマッピングしてから処理している」
        ことが示唆されています。平たく言えば、
        <strong>「言語モデルは内部では英語で考えている」</strong>
        と言ってよい状況です。
      </p>

      <p>
        モデルがほぼ初めて見るような低資源言語を学習する際には、
        その言語から「英語で考えるための内部表現」への経路がまだ整理されていません。
        TRepLiNa はこの点に着目し、
        <strong>初めて見る言語と高資源言語（ここでは主に英語）のあいだの表現類似度</strong>
        を損失関数に含めることで、モデルが「英語で考える」ためのパスを整えることを目指します。
      </p>

      <p>
        結果として、低資源言語の機械翻訳タスクにおいて、
        通常のファインチューニングよりも高い BLEU / chrF++ スコアを得られるケースが示されています
        （ただし効果は言語間の類似度などにも依存します）。
      </p>

      <p>
        本論文では主に
        <strong>低資源言語 → 高資源言語</strong>（例: LRL → 英語）方向の翻訳が中心ですが、
        実験結果からは <strong>高資源言語 → 低資源言語</strong> 方向の翻訳にも
        効果があることが示唆されています。
      </p>
    </section>

    <!-- 仕組み -->
    <section>
      <h2>仕組みの概要</h2>
      <p>
        TRepLiNa では、低資源言語と高資源言語の
        <strong>内部表現の類似度</strong>を制御するために、
        以下の 2 つの技術を組み合わせた損失関数を用います。
      </p>
      <ul>
        <li><strong>REPINA</strong>: ベースモデルと微調整後モデルの表現ドリフトを抑える正則化</li>
        <li><strong>CKA (Centered Kernel Alignment)</strong>: 異なる言語間の表現空間の類似度を測る指標</li>
      </ul>

      <!-- REPINA -->
      <h3>REPINA の仕組み</h3>
      <p>
        REPINA は、もともと
        「微調整後の表現が、ベースモデルの表現からどれだけ逸脱しているか」を制御するための
        <strong>表現保存型の正則化</strong>です。
      </p>

      <h4>一般的なイメージ（線形射影ありの REPINA）</h4>
      <p>
        ある層 \\(\ell\\) のトークン \\(i\\) に対して、
        <ul>
          <li>\\(h_{\ell,i}^{(0)}\\): 微調整前（ベース）の隠れ状態</li>
          <li>\\(h_{\ell,i}^{(\theta)}\\): 微調整後（パラメータ \\(\\theta\\)）の隠れ状態</li>
        </ul>
        とします。REPINA では、ベースの表現空間と微調整後の表現空間のあいだに
        線形写像 \\(W\\) が存在すると仮定し、
        \\(W h_{\ell,i}^{(\theta)}\\) を \\(h_{\ell,i}^{(0)}\\) にできるだけ近づけるように学習します。
      </p>

      <p>
        そのときの REPINA 損失は、典型的には次のように書けます:
      </p>
      <p class="math-note">
        \\[
          \\mathcal{L}_{\\mathrm{REPINA}}
          = \\frac{1}{N}
            \\sum_{i=1}^{N}
              \\left\\lVert
                W h_{\\ell,i}^{(\\theta)} - h_{\\ell,i}^{(0)}
              \\right\\rVert_2^2
        \\]
        ここで \\(N\\) はその層で見ているトークン数、
        \\(\\lVert \\cdot \\rVert_2\\) は L2 ノルムです。
      </p>

      <h4>本実装での REPINA<sup>I</sup>（恒等射影）</h4>
      <p>
        TRepLiNa コードでは、よりシンプルな
        <strong>REPINA<sup>I</sup>（identity projection 版）</strong>
        を採用しており、\\(W = I\\)（恒等行列）とみなしています。このとき損失は
      </p>
      <p class="math-note">
        \\[
          \\mathcal{L}_{\\mathrm{REPINA}^{I}}
          = \\frac{1}{N}
            \\sum_{i=1}^{N}
              \\left\\lVert
                h_{\\ell,i}^{(\\theta)} - h_{\\ell,i}^{(0)}
              \\right\\rVert_2^2
        \\]
      </p>
      <p>
        つまり「<strong>LoRA などで微調整して得られた表現</strong>」と
        「<strong>アダプタを無効化したベース Aya-23 の表現</strong>」が
        あまりにも乖離しないように L2 距離でペナルティを課す、という形になっています。
      </p>

      <!-- CKA -->
      <h3>CKA の仕組み</h3>
      <p>
        CKA (Centered Kernel Alignment) は、
        <strong>2 つの表現行列の「形」や「構造」がどれくらい似ているか</strong>を測る指標です。
        特に線形 CKA は、Kornblith et al. (2019) で提案された、
        比較的単純で扱いやすいバリアントです。
      </p>

      <h4>線形 CKA の定義</h4>
      <p>
        ある層のトークン表現をまとめた行列を
        \\(X \\in \\mathbb{R}^{N \\times D}\\),
        \\(Y \\in \\mathbb{R}^{N \\times D}\\) とします（ここで \\(N\\) はトークン数、\\(D\\) は隠れ次元）。
        行方向（サンプル方向）で平均を引いて中心化したものを
        \\(\\tilde{X}, \\tilde{Y}\\) とすると、
      </p>
      <p class="math-note">
        \\[
          \\mathrm{CKA}(X, Y)
          =
          \\frac{
            \\left\\lVert \\tilde{X}^\\top \\tilde{Y} \\right\\rVert_F^2
          }{
            \\left\\lVert \\tilde{X}^\\top \\tilde{X} \\right\\rVert_F
            \\cdot
            \\left\\lVert \\tilde{Y}^\\top \\tilde{Y} \\right\\rVert_F
          }
        \\]
      </p>
      <p>
        ここで \\(\\lVert \\cdot \\rVert_F\\) はフロベニウスノルムです。
        値は \\(0 \\leq \\mathrm{CKA} \\leq 1\\) に正規化されており、
        <strong>1 に近いほど 2 つの表現空間が似ている</strong>ことを意味します。
      </p>

      <p>
        TRepLiNa では、低資源言語側の表現行列を \\(X\\)、
        高資源言語（ピボット言語）側の表現行列を \\(Y\\) として CKA を計算し、
        <strong>「2 言語の表現が似てほしい」</strong> ので
        \\(1 - \\mathrm{CKA}(X,Y)\\) を損失として最小化します。
      </p>
      <p class="math-note">
        \\[
          \\mathcal{L}_{\\mathrm{CKA}} = 1 - \\mathrm{CKA}(X, Y)
        \\]
      </p>
    </section>

    <!-- 実装の詳細 -->
    <section>
      <h2>実装の詳細（CKA / REPINA / 全体の損失）</h2>

      <p>
        コード全体は GitHub で公開されています：
        <a href="https://github.com/konta3738/cka-repina-aya23" target="_blank" rel="noopener noreferrer">
          https://github.com/konta3738/cka-repina-aya23
        </a><br />
        ここでは、提供していただいたコードから
        <strong>CKA 部分・REPINA 部分・全体の損失関数</strong>に関係する箇所のみを抜き出して説明します。
      </p>

      <!-- CKA 実装 -->
      <h3>1. CKA の実装部分</h3>
      <p>
        線形 CKA は、まず行方向に中心化する <code>center_rows</code> 関数と、
        実際に CKA 値を計算する <code>linear_cka</code> 関数として定義されています。
      </p>

      <pre><code>def center_rows(x: torch.Tensor, mask: Optional[torch.Tensor] = None) -&gt; torch.Tensor:
    """
    Center features along the sample dimension (rows).

    x: [N, D]
    mask: [N] boolean or float mask (1 for keep). If provided, compute mean over masked rows.
    """
    if mask is None:
        mean = x.mean(dim=0, keepdim=True)
        return x - mean
    else:
        if mask.dtype != torch.float32:
            mask = mask.float()
        # avoid division by zero
        denom = mask.sum().clamp(min=1.0)
        mean = (x * mask.unsqueeze(1)).sum(dim=0, keepdim=True) / denom
        return x - mean

def linear_cka(x: torch.Tensor, y: torch.Tensor,
               x_mask: Optional[torch.Tensor] = None,
               y_mask: Optional[torch.Tensor] = None,
               eps: float = 1e-6) -&gt; torch.Tensor:
    """
    Compute linear CKA between two representation matrices.

    x: [N, D]
    y: [M, D]  (we'll downsample/upsample to min(N,M) by truncation to align tokens if not equal)
    masks: optional [N] and [M] masks
    """
    # If different number of rows (tokens), align by truncation to min length.
    n = x.shape[0]
    m = y.shape[0]
    L = min(n, m)
    x = x[:L]
    y = y[:L]
    if x_mask is not None:
        x_mask = x_mask[:L]
    if y_mask is not None:
        y_mask = y_mask[:L]

    # Center rows
    x = center_rows(x, x_mask)
    y = center_rows(y, y_mask)

    # Following Kornblith et al. (2019): CKA = ||X^T Y||_F^2 / (||X^T X||_F * ||Y^T Y||_F)
    # where X, Y are row-centered.
    xty = x.T @ y
    num = (xty * xty).sum()

    xtx = x.T @ x
    yty = y.T @ y
    denom = torch.sqrt((xtx * xtx).sum().clamp(min=eps)) * \
            torch.sqrt((yty * yty).sum().clamp(min=eps))
    return (num / denom).clamp(min=0.0, max=1.0)</code></pre>

      <p>
        ここでやっていることは、数式で説明した線形 CKA とほぼそのまま一致しており、
        中心化された行列 \\(\\tilde{X}, \\tilde{Y}\\) から
        \\(\\tilde{X}^\\top \\tilde{Y}\\), \\(\\tilde{X}^\\top \\tilde{X}\\), \\(\\tilde{Y}^\\top \\tilde{Y}\\)
        を計算し、フロベニウスノルムにより正規化しています。
      </p>

      <p>
        学習時には、低資源言語側表現 <code>a_src</code> と
        高資源言語側表現 <code>b_src</code> に対して CKA を適用し、
        損失として
        <code>cka_loss = 1.0 - cka_val</code> を用いています。
      </p>

      <pre><code># a_src, b_src は &quot;source tokens の表現&quot; を並べた (N, D) テンソル
cka_val = linear_cka(a_src, b_src, None, None)
cka_loss = 1.0 - cka_val</code></pre>

      <!-- REPINA 実装 -->
      <h3>2. REPINA<sup>I</sup> の実装部分</h3>
      <p>
        REPINA<sup>I</sup> は、<code>compute_losses</code> 関数内で
        「アダプタを無効化したベースモデル」との表現距離として実装されています。
      </p>

      <pre><code># 3) REPINA^I (identity projection): L2 between base (adapters disabled)
# and current reps, same inputs as a_align
do_repina = ((global_step + 1) % 2 == 0)  # every 2 steps
if do_repina:
    with torch.no_grad():
        with model.disable_adapter():
            base_out = forward_hidden(model, a_ids, a_attn)

    base_reps_all, _ = gather_layer_hidden(base_out.hidden_states, layer_index, a_attn)
    rep_cur = a_reps_all[a_src_keep]
    rep_pre = base_reps_all[a_src_keep]
    if rep_cur.shape[0] != rep_pre.shape[0]:
        L = min(rep_cur.shape[0], rep_pre.shape[0])
        rep_cur = rep_cur[:L]
        rep_pre = rep_pre[:L]

    repina_loss = torch.mean((rep_cur - rep_pre) ** 2)</code></pre>

      <p>
        処理の流れは次の通りです。
      </p>
      <ol>
        <li>現在の LoRA 付きモデル（微調整中）で、低資源言語側の入力 <code>a_ids</code> を通し、
          層 <code>layer_index</code> の表現 <code>a_reps_all</code> を取得。</li>
        <li><code>model.disable_adapter()</code> を用いてアダプタを無効化し、
          同じ入力 <code>a_ids</code> をベースモデルとして再度前向き計算し、
          <code>base_reps_all</code> を取得。</li>
        <li><code>a_src_keep</code> によってソーストークン部分を抽出し、
          <code>rep_cur</code>（微調整後）と <code>rep_pre</code>（ベース）を対応づける。</li>
        <li>両者の L2 距離の平均を <code>repina_loss</code> として計算。</li>
      </ol>

      <p>
        数式で書けば、REPINA<sup>I</sup> 損失は先ほど示した
        \\(\\mathcal{L}_{\\mathrm{REPINA}^{I}}\\) に対応しており、
        実装ではその平均を PyTorch のテンソルとして計算しています。
      </p>

      <!-- 全体損失 -->
      <h3>3. 全体の損失関数</h3>
      <p>
        全体の損失は <code>compute_losses</code> 関数の中で定義されています。
        ここでは、以下の 3 つの項から構成されます。
      </p>
      <ul>
        <li><strong>タスク損失</strong>（ラベルスムージング付き NLL）</li>
        <li><strong>CKA 損失</strong>（言語間アライメント）</li>
        <li><strong>REPINA<sup>I</sup> 損失</strong>（表現ドリフト抑制、一定ステップごと）</li>
      </ul>

      <pre><code>def compute_losses(
    model,
    batch: Dict[str, torch.Tensor],
    tokenizer,
    layer_index: int,
    lambda_cka: float,
    mu_repina: float,
    bf16: bool,
    global_step
):
    """
    Returns (total_loss, dict of components)
    """
    device = next(model.parameters()).device
    repina_loss = torch.tensor(0.0, device=device)

    # 1) Task loss: LM on prompt+target (labels already mask prompt with -100)
    input_ids = batch[&quot;input_ids&quot;].to(device)
    labels    = batch[&quot;labels&quot;].to(device)
    attention_mask = (input_ids != tokenizer.pad_token_id).to(device)

    outputs = model(
        input_ids=input_ids,
        attention_mask=attention_mask,
        labels=labels,
        use_cache=False,
        output_hidden_states=True
    )
    #task_loss = outputs.loss
    task_loss = compute_task_loss_with_label_smoothing(
        outputs, labels, epsilon=0.1, ignore_index=-100
    )

    # 2) Alignment loss (CKA) on source regions of A and B
    a_ids = batch[&quot;a_align_ids&quot;].to(device)
    b_ids = batch[&quot;b_align_ids&quot;].to(device)
    a_mask = batch[&quot;a_align_mask&quot;].to(device)
    b_mask = batch[&quot;b_align_mask&quot;].to(device)

    a_attn = (a_ids != tokenizer.pad_token_id).to(device)
    b_attn = (b_ids != tokenizer.pad_token_id).to(device)

    a_out = forward_hidden(model, a_ids, a_attn)
    b_out = forward_hidden(model, b_ids, b_attn)

    a_reps_all, a_tokmask_all = gather_layer_hidden(a_out.hidden_states, layer_index, a_attn)
    b_reps_all, b_tokmask_all = gather_layer_hidden(b_out.hidden_states, layer_index, b_attn)

    a_src_keep = a_attn.reshape(-1).bool()
    b_src_keep = b_attn.reshape(-1).bool()

    a_src = a_reps_all[a_src_keep]
    b_src = b_reps_all[b_src_keep]

    # Edge case: if no tokens (shouldn't happen), fall back to all non-pad tokens
    if a_src.numel() == 0:
        a_src = a_reps_all[a_tokmask_all]
    if b_src.numel() == 0:
        b_src = b_reps_all[b_tokmask_all]

    # Now compute CKA on the (N,D) matrices.
    cka_val = linear_cka(a_src, b_src, None, None)
    cka_loss = 1.0 - cka_val

    # 3) REPINA^I (identity projection): L2 between base (adapters disabled)
    # and current reps, same inputs as a_align
    do_repina = ((global_step + 1) % 2 == 0)  # every 2 steps
    if do_repina:
        with torch.no_grad():
            with model.disable_adapter():
                base_out = forward_hidden(model, a_ids, a_attn)
        base_reps_all, _ = gather_layer_hidden(base_out.hidden_states, layer_index, a_attn)
        rep_cur = a_reps_all[a_src_keep]
        rep_pre = base_reps_all[a_src_keep]
        if rep_cur.shape[0] != rep_pre.shape[0]:
            L = min(rep_cur.shape[0], rep_pre.shape[0])
            rep_cur = rep_cur[:L]
            rep_pre = rep_pre[:L]

        repina_loss = torch.mean((rep_cur - rep_pre) ** 2)

        total = task_loss + lambda_cka * cka_loss + mu_repina * repina_loss
    else:
        total = task_loss + lambda_cka * cka_loss

    return total, {
        &quot;task_loss&quot;: task_loss.detach().float().item(),
        &quot;cka&quot;: cka_val.detach().float().item(),
        &quot;cka_loss&quot;: cka_loss.detach().float().item(),
        &quot;repina&quot;: repina_loss.detach().float().item(),
        &quot;total&quot;: total.detach().float().item()
    }</code></pre>

      <h4>数式でのまとめ</h4>
      <p class="math-note">
        タスク損失を \\(\\mathcal{L}_{\\mathrm{task}}\\)、CKA 損失を
        \\(\\mathcal{L}_{\\mathrm{CKA}} = 1 - \\mathrm{CKA}(X,Y)\\)、REPINA<sup>I</sup> 損失を
        \\(\\mathcal{L}_{\\mathrm{REPINA}^{I}}\\) とすると、全体の損失は
        \\[
          \\mathcal{L}_{\\mathrm{total}}
          =
          \\mathcal{L}_{\\mathrm{task}}
          + \\lambda_{\\mathrm{CKA}} \\cdot \\mathcal{L}_{\\mathrm{CKA}}
          + \\mu_{\\mathrm{REPINA}} \\cdot \\mathcal{L}_{\\mathrm{REPINA}^{I}}
        \\]
        となります（実装では REPINA 部分は一定ステップごとにのみ加算）。
      </p>

      <p>
        直感的には、
      </p>
      <ul>
        <li>\\(\\mathcal{L}_{\\mathrm{task}}\\): 翻訳として正しい出力を出すように学習させる</li>
        <li>\\(\\mathcal{L}_{\\mathrm{CKA}}\\): 低資源言語と高資源言語の表現空間の構造を揃える</li>
        <li>\\(\\mathcal{L}_{\\mathrm{REPINA}^{I}}\\): ベースモデルが持つ汎用的な能力から逸脱しすぎないようにする</li>
      </ul>
      <p>
        という 3 つの力が同時に働くように設計されています。
      </p>
    </section>
  </main>

  <footer>
    TRepLiNa / CKA + REPINA チューニング 解説ページ（HTML版）<br />
    ※このページは研究紹介用の静的サイトとして GitHub Pages などでそのまま利用できます。
  </footer>
</body>
</html>
